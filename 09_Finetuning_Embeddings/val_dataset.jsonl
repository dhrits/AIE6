{"questions": {"0b444b59-432c-4a0b-9f60-ff1704f448f7": "What significance did the year 2024 have in relation to the word \"slop\"?  ", "9cf2a150-82c3-4344-8399-c9015c3b4a8f": "Who originally tweeted about the concept of \"slop\" that was later expanded upon in May 2024?", "500884f8-62a4-49d5-b06d-807161b978f8": "What does the term \"slop\" refer to in the context of AI-generated content?  ", "f2b98ab2-0259-40f0-8e1f-57f6ca308059": "How does the author compare the terms \"spam\" and \"slop\" in relation to unwanted content?", "237538de-80e4-43ea-9cd6-157a3ae1c83b": "What does the term \u201cslop\u201d refer to in the context of generative AI usage?  ", "c6b36e31-b805-4996-b8a7-852bad4d7108": "What is \u201cmodel collapse\u201d and when was it first described?", "ea89ed7f-5d6b-449a-94bb-3a59397450a0": "What is the concern about AI models degenerating due to training on AI-generated content?  ", "51a0bcfe-2456-4fd8-b26e-b1a7a1323790": "How are AI labs using synthetic content to improve their models according to the context?", "0fbbddfb-efc3-48cb-a98c-b986b82d26b3": "What role does synthetic data play in the pretraining of the Phi series of models?  ", "f846bc75-c7b4-4b81-9a6d-2a97eb33b391": "How does synthetic data compare to organic data according to the context provided?", "541c2f04-aa70-4155-bf77-1062d4e4d424": "Why is it challenging for a model to learn effectively from next-token prediction in organic datasets?  ", "15f315c3-1a29-4580-b0f6-d396bc04c7da": "How does the nature of token prediction in language models facilitate following reasoning patterns?", "d17cb114-e27c-47cf-a38f-cb43ee117182": "What technique is commonly used by labs to create training data for smaller, cheaper models?  ", "3f4dde19-c63d-4b45-b1c2-aa097033636e": "How has the approach to designing training data for large language models changed compared to earlier methods?", "688714e1-2647-4a66-a6de-942be581aecd": "Why does the author compare LLMs to chainsaws disguised as kitchen knives?  ", "5ad13e5d-f481-4c29-a138-63e5552d420f": "What factors affect the accuracy of answers provided by LLMs according to the context?", "b66febfe-fd8c-4f4b-a4b0-c1cd9bfef022": "What are some of the different tools that various systems can apply to problems according to the context?  ", "b3c02a59-adab-4ad3-90fe-4d9497faa9f0": "Why is it important to understand CSP and CORS HTTP headers when building a Claude Artifact that talks to an external API?", "d577048d-c5c3-4a0e-af34-3b2ae5bd8910": "What are the main limitations of OpenAI\u2019s o1 model despite its improved capabilities?  ", "83f4076c-1a67-4c72-b20a-3f67cc587d5a": "How does the user experience of the default LLM chat UI compare to learning new computer systems?", "fab3a7eb-b7e6-4f6f-aba3-e58ad9b04134": "Why is it considered ludicrous to try to win an argument using a screenshot from ChatGPT?  ", "86f6830f-f852-45a5-acae-898e7a2952dd": "What factors contribute to end users developing inaccurate mental models of how AI models like ChatGPT work?", "2674d9cc-cd5e-4c58-ae81-f5f50adc8cb4": "Why have some better informed people sworn off using LLMs entirely?  ", "8abcad22-3d43-44e3-ae54-508d353b5d66": "What is described as a \"decidedly non-obvious skill\" when working with LLMs?"}, "relevant_contexts": {"0b444b59-432c-4a0b-9f60-ff1704f448f7": ["64abde0e-c985-46f7-aa7f-62e0d11f0377"], "9cf2a150-82c3-4344-8399-c9015c3b4a8f": ["64abde0e-c985-46f7-aa7f-62e0d11f0377"], "500884f8-62a4-49d5-b06d-807161b978f8": ["fbb9320d-4ffe-4174-bc50-1cabd453a404"], "f2b98ab2-0259-40f0-8e1f-57f6ca308059": ["fbb9320d-4ffe-4174-bc50-1cabd453a404"], "237538de-80e4-43ea-9cd6-157a3ae1c83b": ["64fcd097-9e04-4276-91fb-befb11cb42fe"], "c6b36e31-b805-4996-b8a7-852bad4d7108": ["64fcd097-9e04-4276-91fb-befb11cb42fe"], "ea89ed7f-5d6b-449a-94bb-3a59397450a0": ["8eb90bd4-01f8-464c-9cf5-bd785ba1b8b4"], "51a0bcfe-2456-4fd8-b26e-b1a7a1323790": ["8eb90bd4-01f8-464c-9cf5-bd785ba1b8b4"], "0fbbddfb-efc3-48cb-a98c-b986b82d26b3": ["15d4405f-8eed-4f49-bfc2-62dc50aec48b"], "f846bc75-c7b4-4b81-9a6d-2a97eb33b391": ["15d4405f-8eed-4f49-bfc2-62dc50aec48b"], "541c2f04-aa70-4155-bf77-1062d4e4d424": ["b589e783-6684-40e9-82b2-285854e221cf"], "15f315c3-1a29-4580-b0f6-d396bc04c7da": ["b589e783-6684-40e9-82b2-285854e221cf"], "d17cb114-e27c-47cf-a38f-cb43ee117182": ["5232728e-14b9-426b-9203-110717d811e7"], "3f4dde19-c63d-4b45-b1c2-aa097033636e": ["5232728e-14b9-426b-9203-110717d811e7"], "688714e1-2647-4a66-a6de-942be581aecd": ["a5703859-9f9e-4491-a7a1-758762b76ead"], "5ad13e5d-f481-4c29-a138-63e5552d420f": ["a5703859-9f9e-4491-a7a1-758762b76ead"], "b66febfe-fd8c-4f4b-a4b0-c1cd9bfef022": ["a8ebc95c-f913-4fc4-be6a-812943439904"], "b3c02a59-adab-4ad3-90fe-4d9497faa9f0": ["a8ebc95c-f913-4fc4-be6a-812943439904"], "d577048d-c5c3-4a0e-af34-3b2ae5bd8910": ["eb0c3e09-12c8-4744-a839-af196ff3857e"], "83f4076c-1a67-4c72-b20a-3f67cc587d5a": ["eb0c3e09-12c8-4744-a839-af196ff3857e"], "fab3a7eb-b7e6-4f6f-aba3-e58ad9b04134": ["1bc968c6-7a7c-4d17-8be7-d8229485e98f"], "86f6830f-f852-45a5-acae-898e7a2952dd": ["1bc968c6-7a7c-4d17-8be7-d8229485e98f"], "2674d9cc-cd5e-4c58-ae81-f5f50adc8cb4": ["b37ce9fe-f855-4a55-809a-a1ed7d8e2343"], "8abcad22-3d43-44e3-ae54-508d353b5d66": ["b37ce9fe-f855-4a55-809a-a1ed7d8e2343"]}, "corpus": {"64abde0e-c985-46f7-aa7f-62e0d11f0377": "The year of slop\n2024 was the year that the word \"slop\" became a term of art. I wrote about this in May, expanding on this tweet by @deepfates:", "fbb9320d-4ffe-4174-bc50-1cabd453a404": "Watching in real time as \u201cslop\u201d becomes a term of art. the way that \u201cspam\u201d became the term for unwanted emails, \u201cslop\u201d is going in the dictionary as the term for unwanted AI generated content\n\nI expanded that definition a tiny bit to this:\n\nSlop describes AI-generated content that is both unrequested and unreviewed.\n\nI ended up getting quoted talking about slop in both the Guardian and the NY Times. Here\u2019s what I said in the NY TImes:\n\nSociety needs concise ways to talk about modern A.I. \u2014 both the positives and the negatives. \u2018Ignore that email, it\u2019s spam,\u2019 and \u2018Ignore that article, it\u2019s slop,\u2019 are both useful lessons.", "64fcd097-9e04-4276-91fb-befb11cb42fe": "I love the term \u201cslop\u201d because it so succinctly captures one of the ways we should not be using generative AI!\nSlop was even in the running for Oxford Word of the Year 2024, but it lost to brain rot.\nSynthetic training data works great\nAn idea that surprisingly seems to have stuck in the public consciousness is that of \u201cmodel collapse\u201d. This was first described in the paper The Curse of Recursion: Training on Generated Data Makes Models Forget in May 2023, and repeated in Nature in July 2024 with the more eye-catching headline AI models collapse when trained on recursively generated data.", "8eb90bd4-01f8-464c-9cf5-bd785ba1b8b4": "The idea is seductive: as the internet floods with AI-generated slop the models themselves will degenerate, feeding on their own output in a way that leads to their inevitable demise!\nThat\u2019s clearly not happening. Instead, we are seeing AI labs increasingly train on synthetic content\u2014deliberately creating artificial data to help steer their models in the right way.\nOne of the best descriptions I\u2019ve seen of this comes from the Phi-4 technical report, which included this:", "15d4405f-8eed-4f49-bfc2-62dc50aec48b": "Synthetic data as a substantial component of pretraining is becoming increasingly common, and the Phi series of models has consistently emphasized the importance of synthetic data. Rather than serving as a cheap substitute for organic data, synthetic data has several direct advantages over organic data.", "b589e783-6684-40e9-82b2-285854e221cf": "Structured and Gradual Learning. In organic datasets, the relationship between tokens is often complex and indirect. Many reasoning steps may be required to connect the current token to the next, making it challenging for the model to learn effectively from next-token prediction. By contrast, each token generated by a language model is by definition predicted by the preceding tokens, making it easier for a model to follow the resulting reasoning patterns.", "5232728e-14b9-426b-9203-110717d811e7": "Another common technique is to use larger models to help create training data for their smaller, cheaper alternatives\u2014a trick used by an increasing number of labs. DeepSeek v3 used \u201creasoning\u201d data created by DeepSeek-R1. Meta\u2019s Llama 3.3 70B fine-tuning used over 25M synthetically generated examples.\nCareful design of the training data that goes into an LLM appears to be the entire game for creating these models. The days of just grabbing a full scrape of the web and indiscriminately dumping it into a training run are long gone.\nLLMs somehow got even harder to use", "a5703859-9f9e-4491-a7a1-758762b76ead": "A drum I\u2019ve been banging for a while is that LLMs are power-user tools\u2014they\u2019re chainsaws disguised as kitchen knives. They look deceptively simple to use\u2014how hard can it be to type messages to a chatbot?\u2014but in reality you need a huge depth of both understanding and experience to make the most of them and avoid their many pitfalls.\nIf anything, this problem got worse in 2024.\nWe\u2019ve built computer systems you can talk to in human language, that will answer your questions and usually get them right! ... depending on the question, and how you ask it, and whether it\u2019s accurately reflected in the undocumented and secret training set.", "a8ebc95c-f913-4fc4-be6a-812943439904": "The number of available systems has exploded. Different systems have different tools they can apply to your problems\u2014like Python and JavaScript and web search and image generation and maybe even database lookups... so you\u2019d better understand what those tools are, what they can do and how to tell if the LLM used them or not.\nDid you know ChatGPT has two entirely different ways of running Python now?\nWant to build a Claude Artifact that talks to an external API? You\u2019d better understand CSP and CORS HTTP headers first.", "eb0c3e09-12c8-4744-a839-af196ff3857e": "The models may have got more capable, but most of the limitations remained the same. OpenAI\u2019s o1 may finally be able to (mostly) count the Rs in strawberry, but its abilities are still limited by its nature as an LLM and the constraints placed on it by the harness it\u2019s running in. o1 can\u2019t run web searches or use Code Interpreter, but GPT-4o can\u2014both in that same ChatGPT UI. (o1 will pretend to do those things if you ask it to, a regression to the URL hallucinations bug from early 2023).\nWhat are we doing about this? Not much. Most users are thrown in at the deep end. The default LLM chat UI is like taking brand new computer users, dropping them into a Linux terminal and expecting them to figure it all out.", "1bc968c6-7a7c-4d17-8be7-d8229485e98f": "Meanwhile, it\u2019s increasingly common for end users to develop wildly inaccurate mental models of how these things work and what they are capable of. I\u2019ve seen so many examples of people trying to win an argument with a screenshot from ChatGPT\u2014an inherently ludicrous proposition, given the inherent unreliability of these models crossed with the fact that you can get them to say anything if you prompt them right.", "b37ce9fe-f855-4a55-809a-a1ed7d8e2343": "There\u2019s a flipside to this too: a lot of better informed people have sworn off LLMs entirely because they can\u2019t see how anyone could benefit from a tool with so many flaws. The key skill in getting the most out of LLMs is learning to work with tech that is both inherently unreliable and incredibly powerful at the same time. This is a decidedly non-obvious skill to acquire!\nThere is so much space for helpful education content here, but we need to do do a lot better than outsourcing it all to AI grifters with bombastic Twitter threads.\nKnowledge is incredibly unevenly distributed\nMost people have heard of ChatGPT by now. How many have heard of Claude?"}}