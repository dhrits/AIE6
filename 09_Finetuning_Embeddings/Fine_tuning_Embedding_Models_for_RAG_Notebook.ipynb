{"cells": [{"cell_type": "markdown", "metadata": {"id": "ckbbj5diaHkg"}, "source": ["# Fine-tuning Embeddings for RAG on Specific Data\n", "\n", "As we start our \"fine-tuning\" week, we'll start with the lowest hanging improvement one can do for RAG - which is:\n", "\n", "Fine-tuning embeddings!\n", "\n", "- \ud83e\udd1d Breakout Room #1:\n", "  - Task 1: Dependencies and Boilerplate\n", "  - Task 2: Loading Data\n", "  - Task 3: Constructing a Fine-tuning Dataset\n", "  - Task 4: Fine-tuning `snowflake-arctic-embed-l`\n", "  - Task 5: Evaluating our Retriever\n", "\n"]}, {"cell_type": "markdown", "metadata": {"id": "2xwor_3X6ODX"}, "source": ["#### Basic Overview of Fine-tuning Embeddings\n", "\n", "In essence, what we want to do when we fine-tune our embedding models is very simple:\n", "\n", "```\n", "Move the embeddings for questions relating to a document\n", "closer together with that document\n", "```\n", "\n", "We can think of fine-tuning our embedding models as follows:\n", "\n", "1) We have some pair of text items that *should* be closer together\n", "  - `Question`, `Document` pairs\n", "  - EX: `Who drives the bus?`, `The bus was driven by Kyle, the Bus Driver`.\n", "\n", "2) We use these pairs as labeled data to fine-tune our embedding model.\n", "\n", "The process of training helps the model more accurately associate our questions with the correct documents."]}, {"cell_type": "markdown", "metadata": {"id": "DX5R3HVz6FOQ"}, "source": ["##### \u2753 Question #1:\n", "\n", "Describe the nuance between using Q&D pairs to train the embedding model vs. inter-document pairs/related sentences.\n", "\n", "What caveats does this approach have? Are there any special considerations for what kind of Q's we should use?\n", "\n", "Typically, Q&D pairs tend to be different from inter-document pairs/related sentences in many ways.\n", "\n", "1. In the case of Q&D pairs, the question tends to be short and generally doesn't hold too much information, whereas the answer or the document tends to be much longer and holds more information.\n", "2. In the case of inter-document pairs/related sentences, the pairs have similar length and information content within them.\n", "3. Any embedding model must thus be tuned to better be able to learn a feature space where a question and its answer are closer in embedding space. This is a harder task than relying on two documents with similar information content."]}, {"cell_type": "markdown", "metadata": {"id": "-NkSaurzbpyS"}, "source": ["## Task 1: Dependencies and Boilerplate\n", "\n", "We'll set up our `nest_asyncio` so we can leverage async loops in our Notebook.\n", "\n", "We'll also install the required libraries we'll be using today, and set up our OpenAI API key!"]}, {"cell_type": "markdown", "metadata": {"id": "9c_EUibmcDU3"}, "source": ["### Nest Asyncio"]}, {"cell_type": "code", "execution_count": 1, "metadata": {"id": "zq-6s7LbPnKH"}, "outputs": [], "source": ["import nest_asyncio\n", "\n", "nest_asyncio.apply()"]}, {"cell_type": "markdown", "metadata": {"id": "R8uFz8RVcFFu"}, "source": ["### Install Dependencies\n", "\n", ">> NOTE: You do not need to do these steps if you are running this notebook locally with `uv`."]}, {"cell_type": "code", "execution_count": 30, "metadata": {"id": "ulZIBA1ZoSsV", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "0f42bcb7-af87-4f08-9f9c-46362648b86c"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/175.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m175.7/175.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[?25h\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/45.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[?25h\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/71.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[?25h"]}], "source": ["!pip install -qU langchain_openai langchain_huggingface langchain_core langchain langchain_community langchain-text-splitters ragas==0.2.10 rapidfuzz"]}, {"cell_type": "code", "execution_count": 3, "metadata": {"id": "3GFD7B-tOCrx", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "f6eba20f-56b9-4c23-ad7a-7cee422735c2"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m103.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m169.4/169.4 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[?25h"]}], "source": ["!pip install -qU faiss-cpu python-pptx==1.0.2 nltk==3.9.1 pymupdf beautifulsoup4 lxml"]}, {"cell_type": "markdown", "metadata": {"id": "0FM-eUlrcI8a"}, "source": ["### Provide OpenAI API Key"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "wA_mlurVqtrp", "outputId": "d77aa6ac-6251-4036-ce7c-ada1ffbe8932"}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Enter Your OpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"]}], "source": ["import os\n", "import getpass\n", "\n", "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter Your OpenAI API Key: \")"]}, {"cell_type": "markdown", "metadata": {"id": "TFZ217gCDVTr"}, "source": ["## Task 2: Loading Data\n", "\n", "We'll prepare our data - and download our webpages which we'll be using for our data today.\n", "\n", "These webpages are from [Simon Willison's](https://simonwillison.net/) yearly \"AI learnings\".\n", "\n", "- [2023 Blog](https://simonwillison.net/2023/Dec/31/ai-in-2023/)\n", "- [2024 Blog](https://simonwillison.net/2024/Dec/31/llms-in-2024/)\n", "\n", "Let's start by collecting our data into a useful pile!"]}, {"cell_type": "code", "execution_count": 5, "metadata": {"id": "wzy7qr5MoEiW"}, "outputs": [], "source": ["!mkdir data"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"id": "eldIwTpioEiW", "outputId": "5c9b4f02-1161-422e-c71a-72c540795f14", "colab": {"base_uri": "https://localhost:8080/"}}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n", "                                 Dload  Upload   Total   Spent    Left  Speed\n", "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 31528    0 31528    0     0   310k      0 --:--:-- --:--:-- --:--:--  311k\n"]}], "source": ["!curl https://simonwillison.net/2023/Dec/31/ai-in-2023/ -o data/2023_llms.html"]}, {"cell_type": "code", "execution_count": 7, "metadata": {"id": "siqft8w6oEiW", "outputId": "698996f4-7662-4abb-d26d-efebe29071ab", "colab": {"base_uri": "https://localhost:8080/"}}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n", "                                 Dload  Upload   Total   Spent    Left  Speed\n", "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 70695    0 70695    0     0   859k      0 --:--:-- --:--:-- --:--:--  862k\n"]}], "source": ["!curl https://simonwillison.net/2024/Dec/31/llms-in-2024/ -o data/2024_llms.html"]}, {"cell_type": "code", "execution_count": 8, "metadata": {"id": "DHJhTzsvN75t"}, "outputs": [], "source": ["from langchain_community.document_loaders import DirectoryLoader\n", "from langchain_community.document_loaders import BSHTMLLoader\n", "\n", "path = \"data/\"\n", "text_loader = DirectoryLoader(path, glob=\"*.html\", loader_cls=BSHTMLLoader)"]}, {"cell_type": "markdown", "metadata": {"id": "-UbKa6-V0nvp"}, "source": ["Next, we'll set up a classic naive chunking strategy as we only care that the documents get parsed into chunks that we can generate synthetic questions about."]}, {"cell_type": "code", "execution_count": 9, "metadata": {"id": "NsPrOOqXOsNX"}, "outputs": [], "source": ["from langchain_text_splitters import RecursiveCharacterTextSplitter\n", "\n", "text_splitter = RecursiveCharacterTextSplitter(\n", "    chunk_size = 750,\n", "    chunk_overlap  = 20,\n", "    length_function = len\n", ")"]}, {"cell_type": "markdown", "metadata": {"id": "lf_PoX7l09Rg"}, "source": ["Next we can load/split these documents as follows.\n", "\n", "> NOTE: You may need to run this cell twice to get it to work."]}, {"cell_type": "code", "execution_count": 10, "metadata": {"id": "OMYPX6N6Os8M"}, "outputs": [], "source": ["training_documents = text_splitter.split_documents(text_loader.load())"]}, {"cell_type": "code", "execution_count": 11, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "PAozuMoNOvnp", "outputId": "a37b9496-f4b0-4bb4-ab45-1a8c72a21c96"}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["102"]}, "metadata": {}, "execution_count": 11}], "source": ["len(training_documents)"]}, {"cell_type": "markdown", "metadata": {"id": "0yE2TFIq1BuJ"}, "source": ["Next, we're going to associate each of our chunks with a unique identifier."]}, {"cell_type": "code", "execution_count": 12, "metadata": {"id": "AwyIForybIpo"}, "outputs": [], "source": ["import uuid\n", "\n", "id_set = set()\n", "\n", "for document in training_documents:\n", "  id = str(uuid.uuid4())\n", "  while id in id_set:\n", "    id = uuid.uuid4()\n", "  id_set.add(id)\n", "  document.metadata[\"id\"] = id"]}, {"cell_type": "markdown", "metadata": {"id": "PJnL4oNg341U"}, "source": ["Next, we'll simply use naive Python slicing to create a training, test, and validation set to prepare our data for the next step."]}, {"cell_type": "code", "execution_count": 13, "metadata": {"id": "MTS4GTSEcnG4"}, "outputs": [], "source": ["training_split_documents = training_documents[:len(training_documents) - 24]\n", "val_split_documents = training_documents[len(training_documents) - 24:102-12]\n", "test_split_documents = training_documents[102-12:]"]}, {"cell_type": "markdown", "metadata": {"id": "tzlvKbONDWvQ"}, "source": ["## Task 3: Constructing a Fine-tuning Dataset\n", "\n", "Using the nodes we created above, we can finally start constructing a fine-tuning dataset utilizing OpenAI's `gpt-4.1-mini`\n", "\n", "The basic idea here is straightforward enough:\n", "\n", "1. We look at a document\n", "2. We generate questions that could be answered by that node\n", "\n", "This gives us a number of question/context pairs that we can use to fine-tune our Embeddings model."]}, {"cell_type": "code", "execution_count": 14, "metadata": {"id": "_EWfmIscMrvg"}, "outputs": [], "source": ["from langchain_openai import ChatOpenAI\n", "\n", "qa_chat_model = ChatOpenAI(\n", "    model=\"gpt-4.1-mini\",\n", "    temperature=0\n", ")"]}, {"cell_type": "markdown", "metadata": {"id": "8-hLnsSB6Y-S"}, "source": ["We'll create a simple Question Generation prompt to query `gpt-4o-mini` to generate Questions for each retrieved context."]}, {"cell_type": "code", "execution_count": 15, "metadata": {"id": "diEWcw00NMSj"}, "outputs": [], "source": ["from langchain_core.prompts import ChatPromptTemplate\n", "\n", "qa_prompt = \"\"\"\\\n", "Given the following context, you must generate questions based on only the provided context.\n", "\n", "You are to generate {n_questions} questions which should be provided in the following format:\n", "\n", "1. QUESTION #1\n", "2. QUESTION #2\n", "...\n", "\n", "Context:\n", "{context}\n", "\"\"\"\n", "\n", "qa_prompt_template = ChatPromptTemplate.from_template(qa_prompt)"]}, {"cell_type": "markdown", "metadata": {"id": "u87Izpgm6_fk"}, "source": ["We'll create a simple chain to query the LLM!"]}, {"cell_type": "code", "execution_count": 16, "metadata": {"id": "ggl9SSjiNbpG"}, "outputs": [], "source": ["question_generation_chain = qa_prompt_template | qa_chat_model"]}, {"cell_type": "markdown", "metadata": {"id": "4duvHirh7DQv"}, "source": ["There's a lot going on in this function - let's take a deeper look:\n", "\n", "1. First, we provide a list of documents and a number of questions\n", "2. We, for each document in our list, generate `n_questions` of questions.\n", "3. We then associate those questions and contexts via a `UUID`.\n", "\n", "> NOTE: The reason we're doing this `UUID` association is for ease of use later in the notebook."]}, {"cell_type": "markdown", "metadata": {"id": "1Lm2JvgC9X37"}, "source": ["##### \ud83c\udfd7\ufe0f Activity #1:\n", "\n", "We have:\n", "\n", "- Lists of `Documents` with the `metadata` field `id`.\n", "\n", "We need:\n", "\n", "- An object with key `id`, which have values `str` questions.\n", "- An object with key `question_id`, which have values `List(str)` which will be a list of associated `context_id`.\n", "\n", "An Example:\n", "\n", "question_object:\n", "```python\n", "{\n", "'b4b95fb6-f827-4454-aa5b-20e62733f172': 'What types of accessible formats are available for persons with disabilities?',\n", "'df58ee4f-714c-419e-8324-94e5870574e2': 'How do accessible formats benefit persons with disabilities?',\n", "'505fce8b-0e56-48de-a251-61027e396918': 'What are some of the risks associated with the increasing capabilities of AI systems that generate synthetic content?',\n", "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': 'Why is it important for providers of AI systems to embed technical solutions for marking and detecting synthetic content?'\n", "}\n", " ```\n", "\n", " context_object:\n", " ```python\n", "{\n", "'b4b95fb6-f827-4454-aa5b-20e62733f172': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n", "'df58ee4f-714c-419e-8324-94e5870574e2': ['dd75bf94-75f3-4603-8e4b-5522f6925638'],\n", "'505fce8b-0e56-48de-a251-61027e396918': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n", "'8ff0ab33-60dc-4fee-8958-91bfb686aca8': ['ffe3893f-688c-48e8-90bd-7a9feb953d90'],\n", "}\n", " ```\n", "\n", " As you can see, a piece of context can be associated with more than 1 question.\n", "\n", " The task is to write the Python function(s) to accomplish this task.\n", "\n", " Your function signature is provided below, along with the desired return values.\n", "\n", " > NOTE: You can make any modifications that you desire - assuming that you have the correct input and outputs."]}, {"cell_type": "code", "execution_count": 34, "metadata": {"id": "U4yi4NfTCnLc"}, "outputs": [], "source": ["import tqdm\n", "import asyncio\n", "\n", "\"\"\"\n", "Sample Usage of TQDM:\n", "\n", "for i in tqdm.tqdm(range(10)):\n", "  time.sleep(1)\n", "\"\"\"\n", "\n", "async def create_questions(documents, n_questions):\n", "\n", "    questions = {}\n", "    relevant_docs = {}\n", "    question_pattern = r\"^\\d+\\.\\s(.*)$\"\n", "    id_set = set()\n", "    ### YOUR CODE HERE\n", "    for i, doc in tqdm.tqdm(enumerate(documents)):\n", "        r = question_generation_chain.invoke({'n_questions': n_questions, \"context\": doc.page_content})\n", "        import re\n", "        import uuid\n", "        question_list = re.findall(question_pattern, r.content, re.MULTILINE)\n", "        for question in question_list:\n", "            id = uuid.uuid4()\n", "            while id in id_set:\n", "                id = uuid()\n", "            id_set.add(id)\n", "            questions[str(id)] = question\n", "            if id not in relevant_docs:\n", "                relevant_docs[str(id)] = [str(doc.metadata[\"id\"])]\n", "            else:\n", "                relevant_docs[id].append(str(doc.metadata[\"id\"]))\n", "\n", "    return questions, relevant_docs"]}, {"cell_type": "markdown", "metadata": {"id": "5W0eWOUo4QGL"}, "source": ["### REMOVE `await` IF NOT USING ASYNC (HINT: Use `async`)"]}, {"cell_type": "code", "execution_count": 75, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "85Dq6KRqEs0F", "outputId": "a9277a0f-4b58-4bf6-b84a-fbbf79915793"}, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["78it [01:38,  1.26s/it]\n"]}], "source": ["training_questions, training_relevant_contexts = await create_questions(training_split_documents, 2)"]}, {"cell_type": "markdown", "metadata": {"id": "_FSTG0bb7w73"}, "source": ["We'll use the function to generate training, validation, and test data."]}, {"cell_type": "code", "execution_count": 76, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "eIZm4CqGVzBx", "outputId": "00304a09-ed33-493b-81c0-a3ed0e9c0869"}, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["12it [00:14,  1.24s/it]\n"]}], "source": ["val_questions, val_relevant_contexts = await create_questions(val_split_documents, 2)"]}, {"cell_type": "code", "execution_count": 77, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "o6qUHg9sV2_y", "outputId": "464f6656-63ed-48a7-885f-219d2d571ea1"}, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["12it [00:13,  1.10s/it]\n"]}], "source": ["test_questions, test_relevant_contexts = await create_questions(test_split_documents, 2)"]}, {"cell_type": "markdown", "metadata": {"id": "K_jYOnAI43zK"}, "source": ["### Reformating and Saving Datasets\n", "\n", "Now, we can save our datasets for later use!"]}, {"cell_type": "code", "execution_count": 78, "metadata": {"id": "iF6IFFq9VsNu"}, "outputs": [], "source": ["import json\n", "\n", "training_corpus = {train_item.metadata[\"id\"] : train_item.page_content for train_item in training_split_documents}\n", "\n", "train_dataset = {\n", "    \"questions\" : training_questions,\n", "    \"relevant_contexts\" : training_relevant_contexts,\n", "    \"corpus\" : training_corpus\n", "}\n", "\n", "with open(\"training_dataset.jsonl\", \"w\") as f:\n", "  json.dump(train_dataset, f)"]}, {"cell_type": "code", "execution_count": 79, "metadata": {"id": "PqF9WaueV-V8"}, "outputs": [], "source": ["val_corpus = {val_item.metadata[\"id\"] : val_item.page_content for val_item in val_split_documents}\n", "\n", "val_dataset = {\n", "    \"questions\" : val_questions,\n", "    \"relevant_contexts\" : val_relevant_contexts,\n", "    \"corpus\" : val_corpus\n", "}\n", "\n", "with open(\"val_dataset.jsonl\", \"w\") as f:\n", "  json.dump(val_dataset, f)"]}, {"cell_type": "code", "execution_count": 80, "metadata": {"id": "0DSQ7WMnWAu6"}, "outputs": [], "source": ["test_corpus = {test_item.metadata[\"id\"] : test_item.page_content for test_item in test_split_documents}\n", "\n", "test_dataset = {\n", "    \"questions\" : test_questions,\n", "    \"relevant_contexts\" : test_relevant_contexts,\n", "    \"corpus\" : test_corpus\n", "}\n", "\n", "with open(\"test_dataset.jsonl\", \"w\") as f:\n", "  json.dump(test_dataset, f)"]}, {"cell_type": "code", "source": ["import json\n", "with open(\"training_dataset.jsonl\", \"r\") as f:\n", "  train_dataset = json.load(f)\n", "\n", "with open(\"val_dataset.jsonl\", \"r\") as f:\n", "  val_dataset = json.load(f)\n", "\n", "with open(\"test_dataset.jsonl\", \"r\") as f:\n", "  test_dataset = json.load(f)"], "metadata": {"id": "Fz1K3uuAqxnS"}, "execution_count": 18, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "vAwklqQCgVi-"}, "source": ["## Task 4: Fine-tuning `snowflake-arctic-embed-l`\n", "\n", "Now that we have a dataset, let's grab a `sentence-transformers` Embeddings model!\n", "\n", "We'll be using Snowflake's [`snowflake-arctic-embed-l`](https://huggingface.co/Snowflake/snowflake-arctic-embed-l) as a base embeddings model.\n", "\n", "It is a well performing embeddings model by itself, but there's a lot of very specific domain terms and vocabulary in our courpus - so lets fine-tune it and see what that can do for us!\n", "\n", ">> NOTE: Skip installing dependencies if you are running this notebook locally."]}, {"cell_type": "code", "execution_count": 19, "metadata": {"id": "AXzVHP3v1Cno", "colab": {"base_uri": "https://localhost:8080/"}, "outputId": "b89f7a4a-00ef-40d6-e236-5a2dc1786804"}, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/345.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m345.7/345.7 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m42.3/42.3 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n", "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n", "cudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\n", "pylibcudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\u001b[0m\u001b[31m\n", "\u001b[0m"]}], "source": ["!pip install -qU sentence_transformers datasets pyarrow"]}, {"cell_type": "code", "execution_count": 20, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 496, "referenced_widgets": ["d0513a55cc9544c7bfeecad2e7a75114", "b9ee5d9b819a4a7f9fc63e290146d379", "9c283de095b7482f822d11eff624a96b", "8006769bcd78483b9008383cd7b163d0", "aa5474a7ccfb4da6ae426f3724160e1d", "f5e8909c6790412a9c876eaf1ee81dfe", "f660b2147e3648d888fb1482e890826c", "247d6cf905f943cfbb9294c2f3444500", "adaa9388e1ec4e72a9e02e4f5fb04d93", "e48a90b597cf4831abadb249bcfb5215", "83299bb1deb44955b95467bad36b816f", "cc5a70b5762e4827a6e96530314ef44f", "51a0b260351249348860f54abeceb97d", "f79727bfe20d4f14b686887c21a78533", "a0d1eb56c4c3402283db507be07d32b6", "c6f8e3fa80134554936028e1099bcea1", "75f0e928109147ff9f5f053c45a6cf37", "b6e2820d1c524fd8b2f5fe91f9b135a8", "56b4e10888804872b09fb11eb4df6e5f", "b5a2ced8d5fb4dacb3c1523b103d862b", "1284a312f3b643e88f36af3a90fec9d3", "1368619feb8c494eb65e99519b31e2b2", "4254c5fa5ff143f7bb6ccaace2932782", "de73fd63f0e44ee8bed146f22ba5bba9", "2d0a7ae5c5064ac680ba90f918e74f6f", "5b3deab415344e90b91f2e3ebeb8c454", "ab0e30d0003040bebb58259df46151a3", "37b1bfd594924a199d9be357f9552d2c", "8f26951a41f7403b8280fa5465d047c7", "f773dc1dda43423a9a73d6cad6170040", "8e9a868518e144ac8c83e51ee66dd5ce", "8aa6bdac8cd3423c8a83db13badb8fa0", "064bac2f50114008835d068a131ea83e", "c90f57391e0949929deffb440db11ecb", "085887537a144611a314ff6ef0237f4e", "3b480d0f59254b47bbeda69c6f4f1e1a", "17faea57c7df4437b02719fe0b19ff5f", "e1a9b87f2eb1410cb57e1bd9930c0a20", "892b440976434e3db3ee93b9b9580d31", "7269cbae1dcb4de8af0efe816383279c", "34bf371162e04702a7334044ddf81e5f", "af2ad584c7eb48a4bc2bc56790145a96", "9b6d3cad54ce43bcb6f0eec8de50aef2", "8fdaa0b6e84b4286afb8026700486c1d", "b6bb1f9edb2442a3bcc6f39cd75f392f", "d8ee4ee5b31d4834a5a14d933445b28c", "d789a7b1fb3243d7bddf1510db5d7038", "9632f9986c324e97aa6383827bb1ba5f", "421e923aad6e4540b81bb6cad998dede", "08ea907e91a0400cabcede7233b1b26b", "144f02c078104eb7a04b87c0e46951cc", "d0e92572ce6340e4a719c9b676929620", "7cb68decc5254bc4bbf2e087afcb6a69", "a1df4bfef86f485985b384a004804964", "57acc30afeea40db9d2a17e5d90e03bc", "52e5f2c4530b4c359047e5920a82e9d4", "6c6b8463d9fb45edba9154cb58f3d2c4", "355b6450e0b448d6a2ba1d1ca88862ff", "3f376f4ae3e741ca93f1a66ad4423176", "98eba95d559046a3b45439ac0cad964c", "1acf2d6ecba444208707d81945cbfe16", "5638b99f5c864633808038a08ae03d66", "c23ce432bf6944ec8c29035d46df919b", "4bffeef8e5a64ac3abbd627a7cffa120", "2bb475637e594ccb90de565135878efa", "eb2780f027aa4c8ba0bb821cfc3807b4", "d9b2058618db416fbfe728bb836312eb", "f05c341f2db74fe58757d89912a35f57", "2c5b9f15b907440aae3649a5e5b457d6", "75a5913c7ff34f52975adf5f229d3a44", "1eee396d965b4884a39719aeaf942281", "7be51c72713240b2b6219f45b38cb361", "2d70508c37f74da387edf96e63c0043a", "f4b08f65381042e0a2a2ffa753b33c09", "99bcf2ec57fa4875a27080850898f254", "926ad1a5f79f4cab86beb5d5d87e1b5a", "b3b560f045444e3aa686b0ea88a8e3f4", "924850c02d594076b353a56f28463c4e", "875d10738a8748de91a2fbe323d5a8db", "d99dccfc48dc4952ba0d6c4dbb1cdde8", "c323cb45cb474d4d825cabe5ad82d015", "c2f4bc14cdd94ebe86e92810e05d93b5", "64806ebb1e934503aa516534e3739349", "dab1eb9270cf4a6b935fb439e6be0a04", "117ad3c068694559b67aec081d1a9854", "563a9dab526443c9b00194694ed2c06b", "28fff4f372a34fb9b3aa91e305043b52", "dda758d7bc3b46d7a93b2a128d1b9076", "db5f42cf0ce54d6ea20693e838021586", "0bc8c3fc67e74a57a3a9e6906c506f89", "ad2c6e7a18f1441da988a3a71c76e137", "7366624e6370497bbc8663a41b18dbbf", "6c2786cbeb4f4d34b52104a480bd2818", "ffaa7ad0f9014d22831bfb0b5e81a2f4", "8a3c607ab2f04880a16d7b167429eefc", "48825f6ebf5b46529b306017e261abb4", "e9ab96c6024c40838bb76032d5695652", "aba17072cfc146bb8aed44f55ec106b1", "3cad1d768ff647f0abfd8feb49b0bd87", "e512b8f768114719bcc3c4a55e87c33d", "cb8aa4eb24534a7bb387e94ab132d02a", "2b460f3c9ca942c885a956221900eb81", "96c964dc18d44f01b1a302ae10722045", "321c0fecc7bc48cb85215e9f29524dda", "3e8b1baf3aca4653a9f6152728c94871", "3e57c5deb01b4aa48369f9b8ab7c5c98", "1757fa16f3ab45b2b06842e9820b0abb", "93ec73c4be0a49fd8341cdc4f180d1c2", "91b624cc415d4289b76f73fcc5022fb4", "688b9d987f0d4e88ab172ccdffc28271", "efa354cf0c9a4b4d9ebc78ec5c949fd9", "59ae15b9df2c4edaa3f138b4aafa166f", "75c3a19cdfb4483286daf8614539c429", "cd8f3b539de643da8f7dedf0313abe54", "e8cfd43696964c549760d36d2a94a925", "b6670026cdd8447c8ff2edb90044fbcd", "7440c0d2656e4dfb850ecfb116064438", "8b24ae80e5654bf4bdfc1a3e3ab6f3ba", "ea22afeec5e2478b972548b6cae4dd7c", "be8f53691ccc41fab5765906355aad31", "cc59b5f38c1f4aa28d73928edd7858a6"]}, "id": "G-PGsQB7Xo6V", "outputId": "fcce49e3-677a-4d61-be65-928c45356819"}, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n", "The secret `HF_TOKEN` does not exist in your Colab secrets.\n", "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n", "You will be able to reuse this secret in all of your notebooks.\n", "Please note that authentication is recommended but still optional to access public models or datasets.\n", "  warnings.warn(\n"]}, {"output_type": "display_data", "data": {"text/plain": ["modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d0513a55cc9544c7bfeecad2e7a75114"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["config_sentence_transformers.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "cc5a70b5762e4827a6e96530314ef44f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["README.md:   0%|          | 0.00/85.4k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4254c5fa5ff143f7bb6ccaace2932782"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["sentence_bert_config.json:   0%|          | 0.00/107 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c90f57391e0949929deffb440db11ecb"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["config.json:   0%|          | 0.00/704 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b6bb1f9edb2442a3bcc6f39cd75f392f"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "52e5f2c4530b4c359047e5920a82e9d4"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer_config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d9b2058618db416fbfe728bb836312eb"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "924850c02d594076b353a56f28463c4e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "db5f42cf0ce54d6ea20693e838021586"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "e512b8f768114719bcc3c4a55e87c33d"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["config.json:   0%|          | 0.00/297 [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "efa354cf0c9a4b4d9ebc78ec5c949fd9"}}, "metadata": {}}], "source": ["from sentence_transformers import SentenceTransformer\n", "\n", "model_id = \"Snowflake/snowflake-arctic-embed-l\"\n", "model = SentenceTransformer(model_id)"]}, {"cell_type": "markdown", "metadata": {"id": "4ztG07iB8CFO"}, "source": ["We'll grab some necessary imports from `sentence_transformers` and `torch`.\n", "\n", "> NOTE: PyTorch (`torch`) is a popular machine learning library - while we don't go very deep into PyTorch it's an incredibly powerful and interesting library! Please read more about it [here](https://pytorch.org/tutorials/beginner/basics/intro.html)!"]}, {"cell_type": "code", "execution_count": 21, "metadata": {"id": "B-WbpuUWYFJr"}, "outputs": [], "source": ["from torch.utils.data import DataLoader\n", "from torch.utils.data import Dataset\n", "from sentence_transformers import InputExample"]}, {"cell_type": "markdown", "metadata": {"id": "AJtPPlck8HBE"}, "source": ["We're using a toy batch size here to reflect the limited number of examples we have.\n", "\n", "> NOTE: It is typical to use a much larger batch size (~64+), hardware permitting."]}, {"cell_type": "code", "execution_count": 22, "metadata": {"id": "8Lokhy6KYHAv"}, "outputs": [], "source": ["BATCH_SIZE = 10"]}, {"cell_type": "markdown", "metadata": {"id": "b-6DT8hc8PmT"}, "source": ["Let's move our dataset into the expected format for training."]}, {"cell_type": "code", "execution_count": 23, "metadata": {"id": "JJk37zQsYJ4P"}, "outputs": [], "source": ["corpus = train_dataset['corpus']\n", "queries = train_dataset['questions']\n", "relevant_docs = train_dataset['relevant_contexts']\n", "\n", "examples = []\n", "for query_id, query in queries.items():\n", "    doc_id = relevant_docs[query_id][0]\n", "    text = corpus[doc_id]\n", "    example = InputExample(texts=[query, text])\n", "    examples.append(example)"]}, {"cell_type": "markdown", "metadata": {"id": "OjFx7KHI8TL0"}, "source": ["Now we can create a `torch` `DataLoader`!"]}, {"cell_type": "code", "execution_count": 24, "metadata": {"id": "tiizmeIqZ_-w"}, "outputs": [], "source": ["loader = DataLoader(\n", "    examples, batch_size=BATCH_SIZE\n", ")"]}, {"cell_type": "markdown", "metadata": {"id": "_vA8rzlX8XbT"}, "source": ["Next up, we'll prepare our loss function!\n", "\n", "Loss is an important part of training, fine-tuning, and more. If you want a deep dive on loss - you can check out our [event on loss!](https://www.youtube.com/watch?v=iB8FWR9aD5Q&t=8s).\n", "\n", "The core loss we're using today is called `MultipleNegativesRankingLoss` - you can find more information [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py).\n", "\n", "This is \"wrapped\" in `MatryoshkaLoss`, which you can read the implementation of [here](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MatryoshkaLoss.py)."]}, {"cell_type": "code", "execution_count": 25, "metadata": {"id": "Uga4nnBqlVeh"}, "outputs": [], "source": ["from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n", "\n", "matryoshka_dimensions = [768, 512, 256, 128, 64]\n", "inner_train_loss = MultipleNegativesRankingLoss(model)\n", "train_loss = MatryoshkaLoss(\n", "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n", ")"]}, {"cell_type": "markdown", "metadata": {"id": "aJG4fOm66PHI"}, "source": ["##### \ud83c\udfd7\ufe0f Activity #2:\n", "\n", "Both of these losses sound \"cool\", but what are they - exactly - under the hood?\n", "\n", "Why are these losses specifically doing? Please write a short summary of each loss.\n", "\n", "> NOTE: This is a course focused on AI Engineering and the application of AI - looking for a hint? Try pasting the code (linked above) into ChatGPT/Claude to write the summary!\n", "\n", "* MultipleNegativeRankingLoss - Is a form of contrative loss. The goal of this objective/loss is to encourage embeddings of related question/context pairs to be as close as possible while pushing apart unrelated question/context pairs. It does so by treating pairs of question/context in the batch as positive examples and treating every other context in the batch as a negative example for the particular question.\n", "\n", "* MatryoshkaLoss - This is a loss based on Matryoshka Representation Learning which creates a nested or hierarchical embedding. In this representation, the full embedding has the best semantic performance, although prefixes of the full embedding can also be used with some loss of some semantic performance."]}, {"cell_type": "markdown", "metadata": {"id": "QKxRuXfH844c"}, "source": ["Now we can set-up our evaluator.\n", "\n", "> NOTE: Due to the formatting of our dataset - this is all we have to do!"]}, {"cell_type": "code", "execution_count": 26, "metadata": {"id": "f0hAFwUyaHQG"}, "outputs": [], "source": ["from sentence_transformers.evaluation import InformationRetrievalEvaluator\n", "\n", "corpus = val_dataset['corpus']\n", "queries = val_dataset['questions']\n", "relevant_docs = val_dataset['relevant_contexts']\n", "\n", "evaluator = InformationRetrievalEvaluator(queries, corpus, relevant_docs)"]}, {"cell_type": "markdown", "metadata": {"id": "MYfap_ct8-bU"}, "source": ["We'll train this model for 5 epochs, though you could increase this number if we had a significant amount more data."]}, {"cell_type": "code", "execution_count": 27, "metadata": {"id": "svZG0pBHiQr6"}, "outputs": [], "source": ["EPOCHS = 10"]}, {"cell_type": "markdown", "metadata": {"id": "wxitWoNX9DwW"}, "source": ["It's training time!\n", "\n", "> NOTE: We're manually defining a warm-up period here - this is just to provide a smooth ramp into our training!"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "O9szndQFoEib", "outputId": "bc45d962-a2fb-4880-caae-61a4c1075d78"}, "outputs": [{"data": {"text/html": ["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/ajb0ecsa?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"], "text/plain": ["<wandb.sdk.wandb_run.Run at 0x7fb5b480cad0>"]}, "execution_count": 39, "metadata": {}, "output_type": "execute_result"}], "source": ["import wandb\n", "wandb.init(mode=\"disabled\")"]}, {"cell_type": "markdown", "metadata": {"id": "Ncv7Qu_CoEib"}, "source": ["> NOTE: You may not see direct improvement during the training cycles - this is absolutely expected. We will verify performance later in the notebook."]}, {"cell_type": "code", "execution_count": 28, "metadata": {"id": "EWTqBGa5oEib"}, "outputs": [], "source": ["import os\n", "os.environ[\"WANDB_DISABLED\"] = \"true\""]}, {"cell_type": "code", "execution_count": 29, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 556, "referenced_widgets": ["650479f8d1ed4fafb506a4a11b20ad97", "da302b5937f748c88868878ec1e73974", "d9a2e64e59ee45d1911bfab9db86e368", "92e968e82b60448d9cb3836f0245324c", "b70a6f2cb9574f5eb9faf67aab2097e9", "58948f9e016743ff954a785ba4946f83", "4440ad1bec3247d3b4026d7357a665ea", "bd6330783f82421c97168839bf66e94f", "bf01ca38b2df46f8965e4fabd3387f3f", "a0d85ddd597f4a6e8ebf9a1b4df43302", "15b005763c1946c2b9e241325432f000"]}, "id": "aDhUHZY-iR09", "outputId": "de0dd2bb-7ece-46a0-c527-a48a2d9c559c"}, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n", "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"]}, {"output_type": "display_data", "data": {"text/plain": ["Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "650479f8d1ed4fafb506a4a11b20ad97"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["<IPython.core.display.HTML object>"], "text/html": ["\n", "    <div>\n", "      \n", "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n", "      [160/160 01:55, Epoch 10/10]\n", "    </div>\n", "    <table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", " <tr style=\"text-align: left;\">\n", "      <th>Step</th>\n", "      <th>Training Loss</th>\n", "      <th>Validation Loss</th>\n", "      <th>Cosine Accuracy@1</th>\n", "      <th>Cosine Accuracy@3</th>\n", "      <th>Cosine Accuracy@5</th>\n", "      <th>Cosine Accuracy@10</th>\n", "      <th>Cosine Precision@1</th>\n", "      <th>Cosine Precision@3</th>\n", "      <th>Cosine Precision@5</th>\n", "      <th>Cosine Precision@10</th>\n", "      <th>Cosine Recall@1</th>\n", "      <th>Cosine Recall@3</th>\n", "      <th>Cosine Recall@5</th>\n", "      <th>Cosine Recall@10</th>\n", "      <th>Cosine Ndcg@10</th>\n", "      <th>Cosine Mrr@10</th>\n", "      <th>Cosine Map@100</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <td>16</td>\n", "      <td>No log</td>\n", "      <td>No log</td>\n", "      <td>0.875000</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.875000</td>\n", "      <td>0.319444</td>\n", "      <td>0.200000</td>\n", "      <td>0.100000</td>\n", "      <td>0.875000</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.945522</td>\n", "      <td>0.927083</td>\n", "      <td>0.927083</td>\n", "    </tr>\n", "    <tr>\n", "      <td>32</td>\n", "      <td>No log</td>\n", "      <td>No log</td>\n", "      <td>0.916667</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.916667</td>\n", "      <td>0.333333</td>\n", "      <td>0.200000</td>\n", "      <td>0.100000</td>\n", "      <td>0.916667</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.969244</td>\n", "      <td>0.958333</td>\n", "      <td>0.958333</td>\n", "    </tr>\n", "    <tr>\n", "      <td>48</td>\n", "      <td>No log</td>\n", "      <td>No log</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.958333</td>\n", "      <td>0.333333</td>\n", "      <td>0.200000</td>\n", "      <td>0.100000</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.984622</td>\n", "      <td>0.979167</td>\n", "      <td>0.979167</td>\n", "    </tr>\n", "    <tr>\n", "      <td>50</td>\n", "      <td>No log</td>\n", "      <td>No log</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.958333</td>\n", "      <td>0.333333</td>\n", "      <td>0.200000</td>\n", "      <td>0.100000</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.984622</td>\n", "      <td>0.979167</td>\n", "      <td>0.979167</td>\n", "    </tr>\n", "    <tr>\n", "      <td>64</td>\n", "      <td>No log</td>\n", "      <td>No log</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.958333</td>\n", "      <td>0.333333</td>\n", "      <td>0.200000</td>\n", "      <td>0.100000</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.984622</td>\n", "      <td>0.979167</td>\n", "      <td>0.979167</td>\n", "    </tr>\n", "    <tr>\n", "      <td>80</td>\n", "      <td>No log</td>\n", "      <td>No log</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.333333</td>\n", "      <td>0.200000</td>\n", "      <td>0.100000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "    </tr>\n", "    <tr>\n", "      <td>96</td>\n", "      <td>No log</td>\n", "      <td>No log</td>\n", "      <td>0.916667</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.916667</td>\n", "      <td>0.333333</td>\n", "      <td>0.200000</td>\n", "      <td>0.100000</td>\n", "      <td>0.916667</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.969244</td>\n", "      <td>0.958333</td>\n", "      <td>0.958333</td>\n", "    </tr>\n", "    <tr>\n", "      <td>100</td>\n", "      <td>No log</td>\n", "      <td>No log</td>\n", "      <td>0.916667</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.916667</td>\n", "      <td>0.333333</td>\n", "      <td>0.200000</td>\n", "      <td>0.100000</td>\n", "      <td>0.916667</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.969244</td>\n", "      <td>0.958333</td>\n", "      <td>0.958333</td>\n", "    </tr>\n", "    <tr>\n", "      <td>112</td>\n", "      <td>No log</td>\n", "      <td>No log</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.958333</td>\n", "      <td>0.333333</td>\n", "      <td>0.200000</td>\n", "      <td>0.100000</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.984622</td>\n", "      <td>0.979167</td>\n", "      <td>0.979167</td>\n", "    </tr>\n", "    <tr>\n", "      <td>128</td>\n", "      <td>No log</td>\n", "      <td>No log</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.958333</td>\n", "      <td>0.333333</td>\n", "      <td>0.200000</td>\n", "      <td>0.100000</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.984622</td>\n", "      <td>0.979167</td>\n", "      <td>0.979167</td>\n", "    </tr>\n", "    <tr>\n", "      <td>144</td>\n", "      <td>No log</td>\n", "      <td>No log</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.958333</td>\n", "      <td>0.333333</td>\n", "      <td>0.200000</td>\n", "      <td>0.100000</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.984622</td>\n", "      <td>0.979167</td>\n", "      <td>0.979167</td>\n", "    </tr>\n", "    <tr>\n", "      <td>150</td>\n", "      <td>No log</td>\n", "      <td>No log</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.958333</td>\n", "      <td>0.333333</td>\n", "      <td>0.200000</td>\n", "      <td>0.100000</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.984622</td>\n", "      <td>0.979167</td>\n", "      <td>0.979167</td>\n", "    </tr>\n", "    <tr>\n", "      <td>160</td>\n", "      <td>No log</td>\n", "      <td>No log</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.958333</td>\n", "      <td>0.333333</td>\n", "      <td>0.200000</td>\n", "      <td>0.100000</td>\n", "      <td>0.958333</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>1.000000</td>\n", "      <td>0.984622</td>\n", "      <td>0.979167</td>\n", "      <td>0.979167</td>\n", "    </tr>\n", "  </tbody>\n", "</table><p>"]}, "metadata": {}}], "source": ["warmup_steps = int(len(loader) * EPOCHS * 0.1)\n", "\n", "model.fit(\n", "    train_objectives=[(loader, train_loss)],\n", "    epochs=EPOCHS,\n", "    warmup_steps=warmup_steps,\n", "    output_path='finetuned_arctic_ft',\n", "    show_progress_bar=True,\n", "    evaluator=evaluator,\n", "    evaluation_steps=50\n", ")"]}, {"cell_type": "code", "execution_count": 95, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 17, "referenced_widgets": ["f7fa051552a14dddbf220433d0b4dd44", "fdc2dd2e2a934dcea6c553deaca52cf7", "f629e8e2f35f4569b917bc5c6194e9e7", "635e4215756a40c2a51cffaee6f5fc04", "77ebf8bfb34f4ef7893a469176d51364", "a008672bceee4ab2b69da08db1aebf2d", "f10654f7586e4fe392627e82e7420fb7", "55917b00c2e8409e8f573dda8363dd24", "edf114f45aa74f2690ab49851d19c194", "150b4aabef53408b90c9f40d98313ebb", "91b259bdeb094174a6a936ff32021ef0", "36d31bd03b4e422faa73e5cea8fefea2", "e0a33309251f405da90d3204530f072d", "18437e998ca948b6b45e985982ac509e", "9703c411ea2a4d5d9df20d1c84109082", "59bea5170e5b446d881554720d8c9515", "6027cd5e4a364b5e8efc622597f93369", "a6b7ea6eb21748958ac550cc4705842f", "c6f379fd7c53467bb87f9a388ba1d59d", "39db5f36ce3847748c26d229d273e636"]}, "id": "b3iwclvyRD8L", "outputId": "75efa42f-e362-4214-9dae-87d33a1843f6"}, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f7fa051552a14dddbf220433d0b4dd44"}}, "metadata": {}}], "source": ["from huggingface_hub import notebook_login\n", "\n", "notebook_login()"]}, {"cell_type": "code", "execution_count": 96, "metadata": {"id": "_pn-Y6yjRoHk"}, "outputs": [], "source": ["hf_username = \"deman539\""]}, {"cell_type": "code", "execution_count": 97, "metadata": {"id": "Nqhf3zWa9AiJ", "colab": {"base_uri": "https://localhost:8080/", "height": 67, "referenced_widgets": ["d7fd31a344df43349749421530577d41", "84606b753d8b42a1be2ab44a9aefc1a6", "9bab4f6c3dd14d14aed39120fb78955b", "e7c98daf756e46d694a3d8a7d3b66b1d", "6007e2009d2143e2b56cec8439b5d329", "92c0e73e75e04e909aca7294fb1ebc51", "65ad819fd3134ec5a704955b2cf1fbe9", "95b188f930be43e89fc9ed34b2f3ce6e", "df3cf8c3bc2c4ac78ceed72089252d81", "a3416d9e8485472791a821d817c64b1e", "d516a9f0e074434eb439fbbf1e27bba3"]}, "outputId": "14b11a0d-b0bc-4971-b371-23754c83f241"}, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d7fd31a344df43349749421530577d41"}}, "metadata": {}}, {"output_type": "execute_result", "data": {"text/plain": ["'https://huggingface.co/deman539/legal-ft-snowflake-l-6baa5389-96fb-4335-bc5a-00df591614a6/commit/a5ccbe6b3564009c2a00c675a4dcd3b81339c4a6'"], "application/vnd.google.colaboratory.intrinsic+json": {"type": "string"}}, "metadata": {}, "execution_count": 97}], "source": ["import uuid\n", "\n", "model.push_to_hub(f\"{hf_username}/legal-ft-snowflake-l-{uuid.uuid4()}\")"]}, {"cell_type": "markdown", "metadata": {"id": "6bo0zW5k9Poq"}, "source": ["## Task 5: Evaluating our Retriever\n", "\n", "Now that we have fine-tuned our retriever - let's see if it's worthwhile!\n", "\n", "We'll start with some basic imports."]}, {"cell_type": "code", "execution_count": 31, "metadata": {"id": "Vq-2oqU0wHFr"}, "outputs": [], "source": ["import pandas as pd\n", "\n", "from langchain_community.vectorstores import FAISS\n", "from langchain_openai.embeddings import OpenAIEmbeddings\n", "from langchain_core.documents import Document"]}, {"cell_type": "markdown", "metadata": {"id": "5jD0qrIh9X8f"}, "source": ["Now we'll define a function that will help us evaluate our retrieval process.\n", "\n", "> NOTE: We're assuming 1 correct document in a \"hit\"."]}, {"cell_type": "code", "execution_count": 32, "metadata": {"id": "0713_3cowX4q"}, "outputs": [], "source": ["def evaluate_openai(\n", "    dataset,\n", "    embed_model,\n", "    top_k=5,\n", "    verbose=False,\n", "):\n", "  corpus = dataset['corpus']\n", "  questions = dataset['questions']\n", "  relevant_docs = dataset['relevant_contexts']\n", "  documents = [Document(page_content=content, metadata={\"id\": doc_id}) for doc_id, content in corpus.items()]\n", "  vectorstore = FAISS.from_documents(documents, embed_model)\n", "\n", "  retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n", "\n", "  eval_results = []\n", "  for id, question in tqdm.tqdm(questions.items()):\n", "    retrieved_nodes = retriever.invoke(question)\n", "    retrieved_ids = [node.metadata[\"id\"] for node in retrieved_nodes]\n", "    expected_id = relevant_docs[id][0]\n", "    is_hit = expected_id in retrieved_ids\n", "    eval_results.append({\"id\": id, \"question\": question, \"expected_id\": expected_id, \"is_hit\": is_hit})\n", "\n", "  return eval_results"]}, {"cell_type": "markdown", "metadata": {"id": "hOr49m4O9lxY"}, "source": ["All that's left to do is evaluate, we'll evaluate our model against:\n", "\n", "1. OpenAI's closed source `text-embedding-3-small`\n", "2. The base non-fine-tuned version of `Snowflake/snowflake-arctic-embed-l`.\n", "\n", "Let's see how it stacks up!"]}, {"cell_type": "markdown", "metadata": {"id": "ijaeYpf593IW"}, "source": ["### `text-embedding-3-small`"]}, {"cell_type": "code", "execution_count": 35, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "kyY3PztaxnU3", "outputId": "94beca92-ec38-43af-83a7-1f5996cb43e4"}, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:18<00:00,  1.32it/s]\n"]}], "source": ["te3_openai = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n", "te3_results = evaluate_openai(test_dataset, te3_openai)"]}, {"cell_type": "code", "execution_count": 36, "metadata": {"id": "kkyW90TCxx_i"}, "outputs": [], "source": ["te3_results_df = pd.DataFrame(te3_results)"]}, {"cell_type": "code", "execution_count": 37, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "MscVRdNCylJ-", "outputId": "bf227ba5-8ad3-4926-ede7-3ea179b148be"}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["np.float64(1.0)"]}, "metadata": {}, "execution_count": 37}], "source": ["te3_hit_rate = te3_results_df[\"is_hit\"].mean()\n", "te3_hit_rate"]}, {"cell_type": "markdown", "metadata": {"id": "4Ra-mh0L96dQ"}, "source": ["### `Snowflake/snowflake-arctic-embed-l` (base)"]}, {"cell_type": "code", "execution_count": 38, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "OEskxwvFypHe", "outputId": "dae8a8a0-648e-4998-e352-4d5886c0785e"}, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:00<00:00, 46.21it/s]\n"]}], "source": ["from langchain_huggingface import HuggingFaceEmbeddings\n", "\n", "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-l\")\n", "arctic_embed_m_results = evaluate_openai(test_dataset, huggingface_embeddings)"]}, {"cell_type": "code", "execution_count": 39, "metadata": {"id": "KlKgiXTWzMTg"}, "outputs": [], "source": ["arctic_embed_m_results_df = pd.DataFrame(arctic_embed_m_results)"]}, {"cell_type": "code", "execution_count": 40, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "zV5vJWrJzOhc", "outputId": "3d8a15b1-b02a-4dbf-b11d-27cdbb29b74c"}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["np.float64(0.75)"]}, "metadata": {}, "execution_count": 40}], "source": ["arctic_embed_m_hit_rate = arctic_embed_m_results_df[\"is_hit\"].mean()\n", "arctic_embed_m_hit_rate"]}, {"cell_type": "markdown", "metadata": {"id": "lcR3-0s19_lu"}, "source": ["### `Snowflake/snowflake-arctic-embed-l` (fine-tuned)"]}, {"cell_type": "code", "execution_count": 41, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "Ilse1LduzP1i", "outputId": "36a7fd13-4c6d-40eb-b4b1-8f05648dccb7"}, "outputs": [{"output_type": "stream", "name": "stderr", "text": ["Some weights of BertModel were not initialized from the model checkpoint at finetuned_arctic_ft and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n", "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n", "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 24/24 [00:00<00:00, 43.40it/s]\n"]}], "source": ["finetune_embeddings = HuggingFaceEmbeddings(model_name=\"finetuned_arctic_ft\")\n", "finetune_results = evaluate_openai(test_dataset, finetune_embeddings)"]}, {"cell_type": "code", "execution_count": 42, "metadata": {"id": "xxhZPqkNzZlh"}, "outputs": [], "source": ["finetune_results_df = pd.DataFrame(finetune_results)"]}, {"cell_type": "code", "execution_count": 43, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "4thAK2BXzaj6", "outputId": "6a190a20-51fb-4ba6-92d9-cbe293911451"}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["np.float64(1.0)"]}, "metadata": {}, "execution_count": 43}], "source": ["finetune_hit_rate = finetune_results_df[\"is_hit\"].mean()\n", "finetune_hit_rate"]}, {"cell_type": "markdown", "metadata": {"id": "iegFM209mBk3"}, "source": ["## Task 1: Vibe Checking the RAG Pipeline\n", "\n", "We're going to use our RAG pipeline to vibe check on some common phrases now that we've modified it!"]}, {"cell_type": "markdown", "metadata": {"id": "Xzg0AA5krgR4"}, "source": ["### Creating New Chunks\n", "\n", "In order to try and evaluate our system more fairly, let's create new chunks that we will use to create our Vector Store."]}, {"cell_type": "code", "execution_count": 110, "metadata": {"id": "KwQ2_LqNr0Tw"}, "outputs": [], "source": ["text_splitter = RecursiveCharacterTextSplitter(\n", "    chunk_size = 600,\n", "    chunk_overlap  = 50,\n", "    length_function = len\n", ")\n", "\n", "training_documents = text_splitter.split_documents(text_loader.load())"]}, {"cell_type": "markdown", "metadata": {"id": "gIdxahHXpP-c"}, "source": ["### Base Chain\n", "\n", "We'll start by constructing our base chain, which will use the untrained retrieval model."]}, {"cell_type": "markdown", "metadata": {"id": "bOsxIXpNpWC2"}, "source": ["#### R - Retrieval"]}, {"cell_type": "code", "execution_count": 111, "metadata": {"id": "azIGIKYfmNCT"}, "outputs": [], "source": ["from langchain_community.vectorstores import FAISS\n", "\n", "base_vectorstore = FAISS.from_documents(training_documents, huggingface_embeddings)\n", "base_retriever = base_vectorstore.as_retriever(search_kwargs={\"k\": 6})"]}, {"cell_type": "markdown", "metadata": {"id": "l-1nVZ0KpX5N"}, "source": ["#### A - Augmented"]}, {"cell_type": "code", "execution_count": 112, "metadata": {"id": "G10Fr-aKojeA"}, "outputs": [], "source": ["from langchain_core.prompts import ChatPromptTemplate\n", "\n", "RAG_PROMPT = \"\"\"\\\n", "Given a provided context and a question, you must answer the question. If you do not know the answer, you must state that you do not know.\n", "\n", "Context:\n", "{context}\n", "\n", "Question:\n", "{question}\n", "\n", "Answer:\n", "\"\"\"\n", "\n", "rag_prompt_template = ChatPromptTemplate.from_template(RAG_PROMPT)"]}, {"cell_type": "markdown", "metadata": {"id": "Euq6RQEopZvD"}, "source": ["#### G - Generation"]}, {"cell_type": "code", "execution_count": 113, "metadata": {"id": "5-mfbbrypMHG"}, "outputs": [], "source": ["rag_llm =  ChatOpenAI(\n", "    model=\"gpt-4.1-nano\",\n", "    temperature=0\n", ")"]}, {"cell_type": "markdown", "metadata": {"id": "wQ2p4mnUpbYY"}, "source": ["#### RAG - LCEL RAG Pipeline"]}, {"cell_type": "code", "execution_count": 114, "metadata": {"id": "ssuR-LaboyGq"}, "outputs": [], "source": ["from operator import itemgetter\n", "from langchain_core.output_parsers import StrOutputParser\n", "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n", "\n", "base_rag_chain = (\n", "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n", "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n", "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n", ")"]}, {"cell_type": "code", "execution_count": 115, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 107}, "id": "emm6WbB9pfKt", "outputId": "2125ab3c-74cf-497f-c825-213318bd5515"}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["'Based on the provided context, an \"agent\" in the context of AI refers to systems that are often described as capable of acting on your behalf, such as travel agents or digital assistants. However, the term is used very vaguely and lacks a clear, widely accepted definition. The discussions highlight that many claims about AI agents are still \"coming soon\" and that their actual utility is questionable due to issues like gullibility and the difficulty of distinguishing truth from fiction. Therefore, an agent generally refers to an AI system purported to perform tasks or make decisions on behalf of a user, but the precise meaning varies and remains somewhat ambiguous.'"], "application/vnd.google.colaboratory.intrinsic+json": {"type": "string"}}, "metadata": {}, "execution_count": 115}], "source": ["base_rag_chain.invoke({\"question\" : \"What is an agent?\"})[\"response\"]"]}, {"cell_type": "code", "execution_count": 116, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 53}, "id": "mUOrd0OBprAq", "outputId": "b840dda8-696c-4ce7-de02-0421248aa332"}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["'Several organizations have produced models that are better than GPT-3, including Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, and Baidu.'"], "application/vnd.google.colaboratory.intrinsic+json": {"type": "string"}}, "metadata": {}, "execution_count": 116}], "source": ["base_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"]}, {"cell_type": "code", "execution_count": 117, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 35}, "id": "OnfuFl59py7I", "outputId": "028be943-f10f-4e85-baf9-3e83bebe9bbc"}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["'The provided context does not specify a particular time of year that is considered the \"laziest\" for AI.'"], "application/vnd.google.colaboratory.intrinsic+json": {"type": "string"}}, "metadata": {}, "execution_count": 117}], "source": ["base_rag_chain.invoke({\"question\" : \"What is the laziest time of the year for AI?\"})[\"response\"]"]}, {"cell_type": "code", "execution_count": 118, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 35}, "id": "-NmqwHBDqTZ8", "outputId": "777ea74d-1d1c-4f07-8d49-5ce9e3397080"}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["'The provided context does not specify the name \"Simon\" or detail the largest model he has run on his phone. Therefore, I do not know the answer.'"], "application/vnd.google.colaboratory.intrinsic+json": {"type": "string"}}, "metadata": {}, "execution_count": 118}], "source": ["base_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"]}, {"cell_type": "markdown", "metadata": {"id": "SqNS0UJAp3lC"}, "source": ["### Fine-tuned Embedding Model\n", "\n", "Now let's rebuild our RAG chain with the Fine-tuned model - the only component we need to change is our `FAISS` vectorstore!"]}, {"cell_type": "code", "execution_count": 119, "metadata": {"id": "ihO7tP6mqATy"}, "outputs": [], "source": ["finetune_vectorstore = FAISS.from_documents(training_documents, finetune_embeddings)\n", "finetune_retriever = finetune_vectorstore.as_retriever(search_kwargs={\"k\": 6})"]}, {"cell_type": "code", "execution_count": 120, "metadata": {"id": "1_cIFvWzqKGY"}, "outputs": [], "source": ["finetune_rag_chain = (\n", "    {\"context\": itemgetter(\"question\") | finetune_retriever, \"question\": itemgetter(\"question\")}\n", "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n", "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n", ")"]}, {"cell_type": "code", "execution_count": 121, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 89}, "id": "OJmRHJF2qNgj", "outputId": "ca95b414-006c-4377-f3a4-5883f413551e"}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["'Based on the provided context, an \"agent\" is a term that lacks a clear, universally accepted definition. It generally refers to systems that act on your behalf, such as travel agents or digital assistants, or to AI systems that have access to tools and can perform tasks in a loop to solve problems. However, the term remains vague and is often associated with concepts like autonomy, which are not precisely defined. The discussion indicates that true \"agents\" as autonomous, reliable entities are still a work in progress and face challenges like gullibility and prompt injection.'"], "application/vnd.google.colaboratory.intrinsic+json": {"type": "string"}}, "metadata": {}, "execution_count": 121}], "source": ["finetune_rag_chain.invoke({\"question\" : \"What is an Agent?\"})[\"response\"]"]}, {"cell_type": "code", "execution_count": 125, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 71}, "id": "EnK-c2ugqPPh", "outputId": "7193fdb8-0a56-4c09-d308-a69181387c43"}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["'According to the provided context, multiple organizations have produced models better than GPT-3. Specifically, the text mentions that \"we\u2019ve now seen better-than-GPT-3 class models produced by Anthropic, Mistral, Google, Meta, EleutherAI, Stability AI, TII in Abu Dhabi (Falcon), Microsoft Research, xAI, Replit, Baidu and a bunch of other organizations.\"'"], "application/vnd.google.colaboratory.intrinsic+json": {"type": "string"}}, "metadata": {}, "execution_count": 125}], "source": ["finetune_rag_chain.invoke({\"question\" : \"Who has produced better models than GPT-3?\"})[\"response\"]"]}, {"cell_type": "code", "execution_count": 123, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 71}, "id": "83hssg1AWozc", "outputId": "6e585ddd-f058-423d-c249-5e7b5d7e03c9"}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["'The provided context suggests that AI, specifically ChatGPT, may become less useful or \"lazy\" in December, possibly because its system prompt includes the current date and training data shows that people provide less useful answers around the holidays. Therefore, the laziest time of the year for AI appears to be December.'"], "application/vnd.google.colaboratory.intrinsic+json": {"type": "string"}}, "metadata": {}, "execution_count": 123}], "source": ["finetune_rag_chain.invoke({\"question\" : \"What is the laziest time of the year for AI?\"})[\"response\"]"]}, {"cell_type": "code", "execution_count": 124, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 35}, "id": "rsHmGeFbqRET", "outputId": "2d3b23ac-926a-4100-e7cb-159a131bccb5"}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["'The largest model that Simon has run on his phone is Mistral 7B.'"], "application/vnd.google.colaboratory.intrinsic+json": {"type": "string"}}, "metadata": {}, "execution_count": 124}], "source": ["finetune_rag_chain.invoke({\"question\" : \"What is the largest model that Simon has run on his phone?\"})[\"response\"]"]}, {"cell_type": "markdown", "metadata": {"id": "jDgD8seY_I3W"}, "source": ["#### \u2753Question #2:\n", "\n", "Which LCEL RAG Chain do you think answered the questions better, and why?\n", "\n", "It is pretty clear that the retriever which used the finetuned embeddings did a better job answering with more specifics based on the documents. As an example, the finetuned retriever pipeline is able to answer that (based on the context), Mistral 7B is the largest model Simon has run on his phone. Additionally, the definition of Agent is also more faithful to the context in the finetuned pipeline."]}, {"cell_type": "markdown", "metadata": {"id": "WCbq1sZArIx4"}, "source": ["## Task 2: RAGAS Evaluation\n", "\n", "It's great to have some idea of how our system is doing based on vibe-checks, but let's use RAGAS to provide more insight info. on how things are improving!\n", "\n", "> NOTE: Please recreate *exactly* the RAGAS process we used to evaluate RAG, baselining with the default retriever, and then comparing the new retriever. The includes the Synthetic Data Generation steps."]}, {"cell_type": "code", "source": ["!pip install rapidfuzz"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "tk5IRC7VCgnc", "outputId": "4d230db5-6d91-432a-ea4c-dc505ba04e1f"}, "execution_count": 45, "outputs": [{"output_type": "stream", "name": "stdout", "text": ["Collecting rapidfuzz\n", "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n", "Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n", "\u001b[?25l   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m\u001b[91m\u2578\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m170.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n", "\u001b[?25hInstalling collected packages: rapidfuzz\n", "Successfully installed rapidfuzz-3.13.0\n"]}]}, {"cell_type": "code", "execution_count": 46, "metadata": {"id": "jq880DtHk9pX", "colab": {"base_uri": "https://localhost:8080/", "height": 363, "referenced_widgets": ["8ebae73b6571458e80b26d3a1e60f598", "b5a79eb6631144ff94ee9dd44d3477e0", "f6f1b5b38bc54b398fe73f69403cf2a0", "aa653fa7f63243b08597eb005113ccc6", "41ac8a3bee6f4bd1a6f89247c9f221b5", "fe01661b28df4f7f9d1e10b851ed006e", "0f09875e750945de825d360d740ae137", "7d7d3ea9e45747e1b61c431f192a8480", "b70d14c74322415fa036f7dd1273cb7b", "103cf351c7544e2c8df39cdd9ded7686", "bbcc91ac2f9345adaec46492210fd290", "4d64001e30c34a23ae42cb53e53f2659", "910f1a7739ad4f228adb4068775bdffd", "7c3ff883bd2a4d1986595b280127318d", "0df2e2d756064af0aeb3e84df3a32bf1", "febd2a0eb3ca4cb593d7ccb6d54c2447", "c90da04d938c49fd85768160863dd383", "7bb2045baeb94c0993d0a79b8c17e569", "aefbde99f35b4c6eb2e7b8d1d977da51", "c8709286eaf34ba0a3360ff3dc1e2c6c", "21bc8dbe6ee9458eba65a70efbb0680c", "ff042cb5f8a84dbca8eb21bc6a02953b", "3ab94a77e821400b87e86f75070180f4", "579082e4998c4eb3b8b9a8c57a25c214", "c4cb6117f7014835ac276f3a2281f3e4", "b08a9a3c516447baaa1b6837c87a1839", "603933e4a0684c0b9e2d2b730f4dc614", "d8f600c4972c48b5bdcef5c285691609", "5c030c778da6426e8a710127b31197f0", "93737bd22ed248a5b5b6cb5114652b8a", "90220c4a7e1a466eb380d29efd07a30a", "9c47e31c28834e1380408a8330548c8a", "c8c6893cd6714664b2e4069fedf21d02", "5cb5f9ba340e4fceac75d1c78844b7b6", "2d322421a076443d86f6e1fc6a464d3b", "922ffd65d3d84b31b4dd42468f0c5a0a", "2b5fd82e650341bfbb8f68101688513f", "f9cf43f3ec3449cc8dd2a3a14e3fe04b", "609a02385bd24af1be37cb928f43e332", "f825e69cf1454801b36d8b6195bd97c9", "96f02a5e5da646059dfe538d3eab863e", "2ebd60cbcb7e40dc91bc2a617e8ed731", "0578451c459c4e20bc50b91a2f4a6a56", "24353e077a6e484382538e0054effe65", "b70463a9765e431589a9a8608ef75541", "224e492f9f7446bcbc0a4f79aeee5254", "cf22e1a197ea44b9a82f0ea072aace91", "6ee395ffd1b64db18d609bdbc059243c", "4693979af4fc41758ade3ea410963bb4", "8203a41f8a2540148116e3c864567743", "3724395c2514445880d2017ae265dbdb", "6335abe65e8740fdb6de66fd70f3da9f", "10c0d369e86f41949831424cf6da078b", "a74c97dd35694ea8a1f4637bd759d3a8", "5c93bedcf2494a4a9feade98fdacfccd", "f7a0c3e1b84f43a591904ab7ae29670e", "caf60074360349b8b0201e1d42f4e195", "7560375f61f941bf921d44be51c9c62f", "8832f8823d36490c863fe7976eb376b2", "827884c4c60e48f59109c26d52b9687e", "faae94d11f294d61b2c33a303ffab3e0", "00f5a9f65c4e41e0957b0775fe83731c", "b6d33ca8f7f241ec9ade0ece7ef67c6d", "5bc4aaceb9334303bcdcfc8a5572b22d", "6c2c40a842c94ca7b8dad976ddd44964", "0dd4e5a235254cb0a81cb85830c56e5d", "17e12884cdcb49108457f10c9828e577", "5fa1d8e3ce814a41ae8b1ab1239dcdd2", "0a7f3d1b0d0c4e01a1901b440df40b36", "a1dac740a0da424e9f167f76d8f8bbd3", "a67de65ffcbe495496707d34392c297c", "727a4105498442d2816a2bba047ee647", "0b10b9a58fb740ae9f144afaf30a7be6", "82ed047d55664577806176a0b4e81cc6", "dd7db290d6c241c2a1902195442821a6", "70a4dc420f8249939328a21f5481b9cc", "d969a9960361498881feda5a6094a61e"]}, "outputId": "39ff23d8-f39f-4602-b057-cf57312403e9"}, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["Applying SummaryExtractor:   0%|          | 0/88 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "8ebae73b6571458e80b26d3a1e60f598"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["Applying CustomNodeFilter:   0%|          | 0/102 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4d64001e30c34a23ae42cb53e53f2659"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["WARNING:ragas.testset.transforms.filters:Node 578046a3-1d4f-40b1-be2a-3e276804e773 does not have a summary. Skipping filtering.\n", "WARNING:ragas.testset.transforms.filters:Node 4baaebd3-c74e-4acd-913b-e1d2b324b95c does not have a summary. Skipping filtering.\n", "WARNING:ragas.testset.transforms.filters:Node f7e60490-fac3-4e43-9d48-c9673952138d does not have a summary. Skipping filtering.\n", "WARNING:ragas.testset.transforms.filters:Node cb2301bb-b2ed-4aaf-ae7c-a8a4e391216e does not have a summary. Skipping filtering.\n", "WARNING:ragas.testset.transforms.filters:Node 323d1152-f2f3-456a-a468-4e6af20e9974 does not have a summary. Skipping filtering.\n", "WARNING:ragas.testset.transforms.filters:Node c2a6be56-5c50-4307-ad83-5f83ea28eaae does not have a summary. Skipping filtering.\n", "WARNING:ragas.testset.transforms.filters:Node 5bf5db7c-cd84-4949-bbeb-9900a65d0826 does not have a summary. Skipping filtering.\n", "WARNING:ragas.testset.transforms.filters:Node 1930cdcf-92ea-4c9d-96d6-37f2df1fc12e does not have a summary. Skipping filtering.\n", "WARNING:ragas.testset.transforms.filters:Node 9966b911-3d71-44ee-9f0f-a0537f274c75 does not have a summary. Skipping filtering.\n", "WARNING:ragas.testset.transforms.filters:Node 85c42f83-f3c9-45f9-9844-9f9d66961f8e does not have a summary. Skipping filtering.\n", "WARNING:ragas.testset.transforms.filters:Node 019fe082-eeeb-45aa-80cd-c1a69862b685 does not have a summary. Skipping filtering.\n", "WARNING:ragas.testset.transforms.filters:Node 5e3b40a7-df6d-425f-84b6-cb93aef8c1b7 does not have a summary. Skipping filtering.\n", "WARNING:ragas.testset.transforms.filters:Node fedea15e-4196-4eb8-a30c-2b533ed36877 does not have a summary. Skipping filtering.\n", "WARNING:ragas.testset.transforms.filters:Node 8f23d1c6-b61a-413b-8b56-6d33ca6a43d5 does not have a summary. Skipping filtering.\n"]}, {"output_type": "display_data", "data": {"text/plain": ["Applying [EmbeddingExtractor, ThemesExtractor, NERExtractor]:   0%|          | 0/289 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3ab94a77e821400b87e86f75070180f4"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "5cb5f9ba340e4fceac75d1c78844b7b6"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b70463a9765e431589a9a8608ef75541"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f7a0c3e1b84f43a591904ab7ae29670e"}}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": ["Generating Samples:   0%|          | 0/10 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "17e12884cdcb49108457f10c9828e577"}}, "metadata": {}}], "source": ["### YOUR CODE HERE\n", "from ragas.testset import TestsetGenerator\n", "\n", "from ragas.llms import LangchainLLMWrapper\n", "from ragas.embeddings import LangchainEmbeddingsWrapper\n", "\n", "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-nano\"))\n", "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n", "\n", "generator = TestsetGenerator(llm=generator_llm, embedding_model=generator_embeddings)\n", "dataset = generator.generate_with_langchain_docs(training_documents, testset_size=10)"]}, {"cell_type": "code", "source": ["dataset.to_pandas()"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 484}, "id": "AbO3hoCP0LmK", "outputId": "93875bc9-c0e1-49f8-bc40-c25b12f891f9"}, "execution_count": 47, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["                                          user_input  \\\n", "0                             What about 2023 in AI?   \n", "1                      Can we build GPT-4 ourselves?   \n", "2                                      LLMs do what?   \n", "3  What are LLMs and why are they considered easy...   \n", "4  So like, if I want to make a big LLM, do I jus...   \n", "5  How does Gemini Ultra compare to Gemini and GP...   \n", "6  December 2023 and December 7th 2023 what happe...   \n", "7  How does Qwen2-VL relate to Qwen and what are ...   \n", "8  Llama 3.2 is better than Llama 3 because it\u2019s ...   \n", "9  How does the Chatbot Arena Leaderboard rank De...   \n", "\n", "                                  reference_contexts  \\\n", "0  [Stuff we figured out about AI in 2023\\n\\n\\n\\n...   \n", "1  [Large Language Models\\nThey\u2019re actually quite...   \n", "2  [Here\u2019s the sequel to this post: Things we lea...   \n", "3  [So far, I think they\u2019re a net positive. I\u2019ve ...   \n", "4  [Intuitively, one would expect that systems th...   \n", "5  [<1-hop>\\n\\nThese abilities are just a few wee...   \n", "6  [<1-hop>\\n\\n260 input tokens, 92 output tokens...   \n", "7  [<1-hop>\\n\\nIn 2024, almost every significant ...   \n", "8  [<1-hop>\\n\\n17th: AI for Data Journalism: demo...   \n", "9  [<1-hop>\\n\\nDeepSeek v3 is a huge 685B paramet...   \n", "\n", "                                           reference  \\\n", "0  2023 was the breakthrough year for Large Langu...   \n", "1              We don\u2019t yet know how to build GPT-4.   \n", "2  Large Language Models (LLMs) can answer questi...   \n", "3  LLMs are large language models that have been ...   \n", "4  Intuitively, one would expect that systems thi...   \n", "5  Gemini Ultra is a new model from Google that h...   \n", "6  In December 2023, Google announced its multi-m...   \n", "7  In 2024, Qwen2-VL was released as part of the ...   \n", "8  Meta\u2019s Llama 3.2 models, including the 3B size...   \n", "9  DeepSeek v3 is ranked 7th on the Chatbot Arena...   \n", "\n", "                       synthesizer_name  \n", "0  single_hop_specifc_query_synthesizer  \n", "1  single_hop_specifc_query_synthesizer  \n", "2  single_hop_specifc_query_synthesizer  \n", "3  single_hop_specifc_query_synthesizer  \n", "4  single_hop_specifc_query_synthesizer  \n", "5  multi_hop_specific_query_synthesizer  \n", "6  multi_hop_specific_query_synthesizer  \n", "7  multi_hop_specific_query_synthesizer  \n", "8  multi_hop_specific_query_synthesizer  \n", "9  multi_hop_specific_query_synthesizer  "], "text/html": ["\n", "  <div id=\"df-c525fd42-ad2e-456f-90c4-36b25527c86a\" class=\"colab-df-container\">\n", "    <div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>user_input</th>\n", "      <th>reference_contexts</th>\n", "      <th>reference</th>\n", "      <th>synthesizer_name</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>0</th>\n", "      <td>What about 2023 in AI?</td>\n", "      <td>[Stuff we figured out about AI in 2023\\n\\n\\n\\n...</td>\n", "      <td>2023 was the breakthrough year for Large Langu...</td>\n", "      <td>single_hop_specifc_query_synthesizer</td>\n", "    </tr>\n", "    <tr>\n", "      <th>1</th>\n", "      <td>Can we build GPT-4 ourselves?</td>\n", "      <td>[Large Language Models\\nThey\u2019re actually quite...</td>\n", "      <td>We don\u2019t yet know how to build GPT-4.</td>\n", "      <td>single_hop_specifc_query_synthesizer</td>\n", "    </tr>\n", "    <tr>\n", "      <th>2</th>\n", "      <td>LLMs do what?</td>\n", "      <td>[Here\u2019s the sequel to this post: Things we lea...</td>\n", "      <td>Large Language Models (LLMs) can answer questi...</td>\n", "      <td>single_hop_specifc_query_synthesizer</td>\n", "    </tr>\n", "    <tr>\n", "      <th>3</th>\n", "      <td>What are LLMs and why are they considered easy...</td>\n", "      <td>[So far, I think they\u2019re a net positive. I\u2019ve ...</td>\n", "      <td>LLMs are large language models that have been ...</td>\n", "      <td>single_hop_specifc_query_synthesizer</td>\n", "    </tr>\n", "    <tr>\n", "      <th>4</th>\n", "      <td>So like, if I want to make a big LLM, do I jus...</td>\n", "      <td>[Intuitively, one would expect that systems th...</td>\n", "      <td>Intuitively, one would expect that systems thi...</td>\n", "      <td>single_hop_specifc_query_synthesizer</td>\n", "    </tr>\n", "    <tr>\n", "      <th>5</th>\n", "      <td>How does Gemini Ultra compare to Gemini and GP...</td>\n", "      <td>[&lt;1-hop&gt;\\n\\nThese abilities are just a few wee...</td>\n", "      <td>Gemini Ultra is a new model from Google that h...</td>\n", "      <td>multi_hop_specific_query_synthesizer</td>\n", "    </tr>\n", "    <tr>\n", "      <th>6</th>\n", "      <td>December 2023 and December 7th 2023 what happe...</td>\n", "      <td>[&lt;1-hop&gt;\\n\\n260 input tokens, 92 output tokens...</td>\n", "      <td>In December 2023, Google announced its multi-m...</td>\n", "      <td>multi_hop_specific_query_synthesizer</td>\n", "    </tr>\n", "    <tr>\n", "      <th>7</th>\n", "      <td>How does Qwen2-VL relate to Qwen and what are ...</td>\n", "      <td>[&lt;1-hop&gt;\\n\\nIn 2024, almost every significant ...</td>\n", "      <td>In 2024, Qwen2-VL was released as part of the ...</td>\n", "      <td>multi_hop_specific_query_synthesizer</td>\n", "    </tr>\n", "    <tr>\n", "      <th>8</th>\n", "      <td>Llama 3.2 is better than Llama 3 because it\u2019s ...</td>\n", "      <td>[&lt;1-hop&gt;\\n\\n17th: AI for Data Journalism: demo...</td>\n", "      <td>Meta\u2019s Llama 3.2 models, including the 3B size...</td>\n", "      <td>multi_hop_specific_query_synthesizer</td>\n", "    </tr>\n", "    <tr>\n", "      <th>9</th>\n", "      <td>How does the Chatbot Arena Leaderboard rank De...</td>\n", "      <td>[&lt;1-hop&gt;\\n\\nDeepSeek v3 is a huge 685B paramet...</td>\n", "      <td>DeepSeek v3 is ranked 7th on the Chatbot Arena...</td>\n", "      <td>multi_hop_specific_query_synthesizer</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>\n", "    <div class=\"colab-df-buttons\">\n", "\n", "  <div class=\"colab-df-container\">\n", "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c525fd42-ad2e-456f-90c4-36b25527c86a')\"\n", "            title=\"Convert this dataframe to an interactive table.\"\n", "            style=\"display:none;\">\n", "\n", "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n", "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n", "  </svg>\n", "    </button>\n", "\n", "  <style>\n", "    .colab-df-container {\n", "      display:flex;\n", "      gap: 12px;\n", "    }\n", "\n", "    .colab-df-convert {\n", "      background-color: #E8F0FE;\n", "      border: none;\n", "      border-radius: 50%;\n", "      cursor: pointer;\n", "      display: none;\n", "      fill: #1967D2;\n", "      height: 32px;\n", "      padding: 0 0 0 0;\n", "      width: 32px;\n", "    }\n", "\n", "    .colab-df-convert:hover {\n", "      background-color: #E2EBFA;\n", "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n", "      fill: #174EA6;\n", "    }\n", "\n", "    .colab-df-buttons div {\n", "      margin-bottom: 4px;\n", "    }\n", "\n", "    [theme=dark] .colab-df-convert {\n", "      background-color: #3B4455;\n", "      fill: #D2E3FC;\n", "    }\n", "\n", "    [theme=dark] .colab-df-convert:hover {\n", "      background-color: #434B5C;\n", "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n", "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n", "      fill: #FFFFFF;\n", "    }\n", "  </style>\n", "\n", "    <script>\n", "      const buttonEl =\n", "        document.querySelector('#df-c525fd42-ad2e-456f-90c4-36b25527c86a button.colab-df-convert');\n", "      buttonEl.style.display =\n", "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n", "\n", "      async function convertToInteractive(key) {\n", "        const element = document.querySelector('#df-c525fd42-ad2e-456f-90c4-36b25527c86a');\n", "        const dataTable =\n", "          await google.colab.kernel.invokeFunction('convertToInteractive',\n", "                                                    [key], {});\n", "        if (!dataTable) return;\n", "\n", "        const docLinkHtml = 'Like what you see? Visit the ' +\n", "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n", "          + ' to learn more about interactive tables.';\n", "        element.innerHTML = '';\n", "        dataTable['output_type'] = 'display_data';\n", "        await google.colab.output.renderOutput(dataTable, element);\n", "        const docLink = document.createElement('div');\n", "        docLink.innerHTML = docLinkHtml;\n", "        element.appendChild(docLink);\n", "      }\n", "    </script>\n", "  </div>\n", "\n", "\n", "    <div id=\"df-639b40c1-33fa-4c13-bb27-75bdc415e503\">\n", "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-639b40c1-33fa-4c13-bb27-75bdc415e503')\"\n", "                title=\"Suggest charts\"\n", "                style=\"display:none;\">\n", "\n", "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n", "     width=\"24px\">\n", "    <g>\n", "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n", "    </g>\n", "</svg>\n", "      </button>\n", "\n", "<style>\n", "  .colab-df-quickchart {\n", "      --bg-color: #E8F0FE;\n", "      --fill-color: #1967D2;\n", "      --hover-bg-color: #E2EBFA;\n", "      --hover-fill-color: #174EA6;\n", "      --disabled-fill-color: #AAA;\n", "      --disabled-bg-color: #DDD;\n", "  }\n", "\n", "  [theme=dark] .colab-df-quickchart {\n", "      --bg-color: #3B4455;\n", "      --fill-color: #D2E3FC;\n", "      --hover-bg-color: #434B5C;\n", "      --hover-fill-color: #FFFFFF;\n", "      --disabled-bg-color: #3B4455;\n", "      --disabled-fill-color: #666;\n", "  }\n", "\n", "  .colab-df-quickchart {\n", "    background-color: var(--bg-color);\n", "    border: none;\n", "    border-radius: 50%;\n", "    cursor: pointer;\n", "    display: none;\n", "    fill: var(--fill-color);\n", "    height: 32px;\n", "    padding: 0;\n", "    width: 32px;\n", "  }\n", "\n", "  .colab-df-quickchart:hover {\n", "    background-color: var(--hover-bg-color);\n", "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n", "    fill: var(--button-hover-fill-color);\n", "  }\n", "\n", "  .colab-df-quickchart-complete:disabled,\n", "  .colab-df-quickchart-complete:disabled:hover {\n", "    background-color: var(--disabled-bg-color);\n", "    fill: var(--disabled-fill-color);\n", "    box-shadow: none;\n", "  }\n", "\n", "  .colab-df-spinner {\n", "    border: 2px solid var(--fill-color);\n", "    border-color: transparent;\n", "    border-bottom-color: var(--fill-color);\n", "    animation:\n", "      spin 1s steps(1) infinite;\n", "  }\n", "\n", "  @keyframes spin {\n", "    0% {\n", "      border-color: transparent;\n", "      border-bottom-color: var(--fill-color);\n", "      border-left-color: var(--fill-color);\n", "    }\n", "    20% {\n", "      border-color: transparent;\n", "      border-left-color: var(--fill-color);\n", "      border-top-color: var(--fill-color);\n", "    }\n", "    30% {\n", "      border-color: transparent;\n", "      border-left-color: var(--fill-color);\n", "      border-top-color: var(--fill-color);\n", "      border-right-color: var(--fill-color);\n", "    }\n", "    40% {\n", "      border-color: transparent;\n", "      border-right-color: var(--fill-color);\n", "      border-top-color: var(--fill-color);\n", "    }\n", "    60% {\n", "      border-color: transparent;\n", "      border-right-color: var(--fill-color);\n", "    }\n", "    80% {\n", "      border-color: transparent;\n", "      border-right-color: var(--fill-color);\n", "      border-bottom-color: var(--fill-color);\n", "    }\n", "    90% {\n", "      border-color: transparent;\n", "      border-bottom-color: var(--fill-color);\n", "    }\n", "  }\n", "</style>\n", "\n", "      <script>\n", "        async function quickchart(key) {\n", "          const quickchartButtonEl =\n", "            document.querySelector('#' + key + ' button');\n", "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n", "          quickchartButtonEl.classList.add('colab-df-spinner');\n", "          try {\n", "            const charts = await google.colab.kernel.invokeFunction(\n", "                'suggestCharts', [key], {});\n", "          } catch (error) {\n", "            console.error('Error during call to suggestCharts:', error);\n", "          }\n", "          quickchartButtonEl.classList.remove('colab-df-spinner');\n", "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n", "        }\n", "        (() => {\n", "          let quickchartButtonEl =\n", "            document.querySelector('#df-639b40c1-33fa-4c13-bb27-75bdc415e503 button');\n", "          quickchartButtonEl.style.display =\n", "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n", "        })();\n", "      </script>\n", "    </div>\n", "\n", "    </div>\n", "  </div>\n"], "application/vnd.google.colaboratory.intrinsic+json": {"type": "dataframe", "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"user_input\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Llama 3.2 is better than Llama 3 because it\\u2019s smaller but still good, right?\",\n          \"Can we build GPT-4 ourselves?\",\n          \"How does Gemini Ultra compare to Gemini and GPT-4, especially considering the recent API improvements and the ongoing development of models that aim to beat GPT-4?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference_contexts\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Meta\\u2019s Llama 3.2 models, including the 3B size, are notable for their efficiency and capability despite not being GPT-4 class. They can run on devices like an iPhone using the MLC Chat iOS app, and at 1B and 3B sizes, they punch above their weight, providing impressive performance in a tiny (<2GB) size. For example, asking it for a plot outline of a Netflix Christmas movie yields a respectable response at 20 tokens per second, demonstrating its usefulness and efficiency compared to larger models like Llama 3.\",\n          \"We don\\u2019t yet know how to build GPT-4.\",\n          \"Gemini Ultra is a new model from Google that has big claims but isn\\u2019t yet available for testing, making it comparable to Gemini in terms of potential. While GPT-4, released by OpenAI in March 2023, has set a high standard, recent API improvements like the easier WebRTC API have made building voice-enabled web apps more accessible. Despite the enormous leaps in AI development this year, no alternative model has yet surpassed GPT-4, though teams like Mistral are working to beat it with significant improvements since September. Therefore, Gemini Ultra\\u2019s current status and upcoming availability are crucial factors in comparing it to Gemini and GPT-4, especially as the AI community continues to develop models aiming to outperform GPT-4.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"synthesizer_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"multi_hop_specific_query_synthesizer\",\n          \"single_hop_specifc_query_synthesizer\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}}, "metadata": {}, "execution_count": 47}]}, {"cell_type": "code", "source": ["from langchain_community.vectorstores import FAISS\n", "from langchain_huggingface import HuggingFaceEmbeddings\n", "\n", "huggingface_embeddings = HuggingFaceEmbeddings(model_name=\"Snowflake/snowflake-arctic-embed-l\")\n", "base_vectorstore = FAISS.from_documents(training_documents, huggingface_embeddings)\n", "base_retriever = base_vectorstore.as_retriever(search_kwargs={\"k\": 6})"], "metadata": {"id": "4NMPjzaQDMLq"}, "execution_count": 54, "outputs": []}, {"cell_type": "code", "source": ["from langchain_core.prompts import ChatPromptTemplate\n", "\n", "RAG_PROMPT = \"\"\"\\\n", "Given a provided context and a question, you must answer the question. If you do not know the answer, you must state that you do not know.\n", "\n", "Context:\n", "{context}\n", "\n", "Question:\n", "{question}\n", "\n", "Answer:\n", "\"\"\"\n", "\n", "rag_prompt_template = ChatPromptTemplate.from_template(RAG_PROMPT)"], "metadata": {"id": "erKxypfMDix-"}, "execution_count": 55, "outputs": []}, {"cell_type": "code", "source": ["rag_llm =  ChatOpenAI(\n", "    model=\"gpt-4.1-nano\",\n", "    temperature=0\n", ")"], "metadata": {"id": "irfQXX_ODlSp"}, "execution_count": 56, "outputs": []}, {"cell_type": "code", "source": ["from operator import itemgetter\n", "from langchain_core.output_parsers import StrOutputParser\n", "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n", "\n", "base_rag_chain = (\n", "    {\"context\": itemgetter(\"question\") | base_retriever, \"question\": itemgetter(\"question\")}\n", "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n", "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n", ")"], "metadata": {"id": "NDemZSKkDpdG"}, "execution_count": 57, "outputs": []}, {"cell_type": "code", "source": ["finetune_embeddings = HuggingFaceEmbeddings(model_name=\"deman539/legal-ft-snowflake-l-6baa5389-96fb-4335-bc5a-00df591614a6\")\n", "finetune_vectorstore = FAISS.from_documents(training_documents, finetune_embeddings)\n", "finetune_retriever = finetune_vectorstore.as_retriever(search_kwargs={\"k\": 6})"], "metadata": {"id": "_sJlpm9MDtG2"}, "execution_count": 52, "outputs": []}, {"cell_type": "code", "source": ["finetune_rag_chain = (\n", "    {\"context\": itemgetter(\"question\") | finetune_retriever, \"question\": itemgetter(\"question\")}\n", "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n", "    | {\"response\": rag_prompt_template | rag_llm | StrOutputParser(), \"context\": itemgetter(\"context\")}\n", ")"], "metadata": {"id": "i64_FhMIEBba"}, "execution_count": 53, "outputs": []}, {"cell_type": "code", "source": ["for test_row in dataset:\n", "  response = base_rag_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n", "  test_row.eval_sample.response = response[\"response\"]\n", "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"], "metadata": {"id": "2kU9EelPEE0z"}, "execution_count": 58, "outputs": []}, {"cell_type": "code", "source": ["from ragas import EvaluationDataset\n", "\n", "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"], "metadata": {"id": "NIC2dtR0GfgG"}, "execution_count": 59, "outputs": []}, {"cell_type": "code", "source": ["from ragas import evaluate\n", "from ragas.llms import LangchainLLMWrapper\n", "\n", "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1\"))"], "metadata": {"id": "ZM2xVh0qGm_V"}, "execution_count": 60, "outputs": []}, {"cell_type": "code", "source": ["from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n", "from ragas import evaluate, RunConfig\n", "\n", "custom_run_config = RunConfig(timeout=360)\n", "\n", "base_result = evaluate(\n", "    dataset=evaluation_dataset,\n", "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n", "    llm=evaluator_llm,\n", "    run_config=custom_run_config\n", ")\n", "base_result"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 123, "referenced_widgets": ["2279dca4b7e44efc999a51e8aa9220c0", "ef800832a6cd4494bcf0d9b632f799bf", "e1000af401da4c51917fe65b0d48703c", "27e2ededc9394b86bf8769867a844917", "290b9a77e0974d3ba1ce14624eb3c7f2", "5d1b243aaa7f4b4183a24912426c4492", "5f8a210301984c91b824b5359cec30cc", "79c66aded9394b2faa21f39585a3ff26", "11d9e67cb5dc417d87a6108221c5fd77", "3292ca7549f94b37b83d815bd324e845", "ac1bf8d434b2417a967801b695bfa20a"]}, "id": "aVTQlsbBGwEz", "outputId": "0d4d78fc-c34b-44a5-8874-35113179f4d1"}, "execution_count": 61, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2279dca4b7e44efc999a51e8aa9220c0"}}, "metadata": {}}, {"output_type": "stream", "name": "stderr", "text": ["ERROR:ragas.executor:Exception raised in Job[38]: TypeError(ufunc 'invert' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe'')\n"]}, {"output_type": "execute_result", "data": {"text/plain": ["{'context_recall': 0.5417, 'faithfulness': 0.7644, 'factual_correctness': 0.3400, 'answer_relevancy': 0.4412, 'context_entity_recall': 0.6185, 'noise_sensitivity_relevant': 0.3479}"]}, "metadata": {}, "execution_count": 61}]}, {"cell_type": "code", "source": ["for test_row in dataset:\n", "  response = finetune_rag_chain.invoke({\"question\" : test_row.eval_sample.user_input})\n", "  test_row.eval_sample.response = response[\"response\"]\n", "  test_row.eval_sample.retrieved_contexts = [context.page_content for context in response[\"context\"]]"], "metadata": {"id": "B6r0k5lkG2XU"}, "execution_count": 62, "outputs": []}, {"cell_type": "code", "source": ["from ragas import EvaluationDataset\n", "\n", "evaluation_dataset = EvaluationDataset.from_pandas(dataset.to_pandas())"], "metadata": {"id": "U4a2Po8xHfhC"}, "execution_count": 63, "outputs": []}, {"cell_type": "code", "source": ["from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness, ResponseRelevancy, ContextEntityRecall, NoiseSensitivity\n", "from ragas import evaluate, RunConfig\n", "\n", "custom_run_config = RunConfig(timeout=360)\n", "\n", "finetune_result = evaluate(\n", "    dataset=evaluation_dataset,\n", "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness(), ResponseRelevancy(), ContextEntityRecall(), NoiseSensitivity()],\n", "    llm=evaluator_llm,\n", "    run_config=custom_run_config\n", ")\n", "finetune_result"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 85, "referenced_widgets": ["9ae70bd2060843308f9ade3bf19a6fc7", "f2b1dd2282ae40df8c85fe62eb955451", "b2032a1fa52b44629b288229c4dbea71", "924a0e194a4149829df18dba7ea8438e", "11919ab9854e459d85cc77e84bbc6e34", "7c24646327bd44dbb33d060add910be1", "b88355f67d1c497293e3c046c9a14cee", "5750903427e1491181d43bffe814c4cb", "50e97881568a44d08199a0d3e5d4ddb9", "56f0fe405d2f470cb6f91214dfd3e720", "0c16adb85e284432ad3b67579accb2f2"]}, "id": "SKGToIeqHjwW", "outputId": "ccc22d2b-8f97-4941-9f40-e9d243eb2c77"}, "execution_count": 64, "outputs": [{"output_type": "display_data", "data": {"text/plain": ["Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"], "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9ae70bd2060843308f9ade3bf19a6fc7"}}, "metadata": {}}, {"output_type": "execute_result", "data": {"text/plain": ["{'context_recall': 0.8250, 'faithfulness': 0.8179, 'factual_correctness': 0.4530, 'answer_relevancy': 0.5461, 'context_entity_recall': 0.7672, 'noise_sensitivity_relevant': 0.3170}"]}, "metadata": {}, "execution_count": 64}]}, {"cell_type": "code", "source": ["comparison = pd.DataFrame([\n", "    {'context_recall': 0.5417, 'faithfulness': 0.7644, 'factual_correctness': 0.3400, 'answer_relevancy': 0.4412, 'context_entity_recall': 0.6185, 'noise_sensitivity_relevant': 0.3479},\n", "    {'context_recall': 0.8250, 'faithfulness': 0.8179, 'factual_correctness': 0.4530, 'answer_relevancy': 0.5461, 'context_entity_recall': 0.7672, 'noise_sensitivity_relevant': 0.3170}\n", "])\n", "comparison.index = [\"Base Retriever\", \"Finetuned Retriever\"]"], "metadata": {"id": "dZtyoVF6Hp2I"}, "execution_count": 70, "outputs": []}, {"cell_type": "code", "source": ["comparison"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 125}, "id": "odC2ZedXIb2Y", "outputId": "5588dc9f-2b08-45d5-b0f4-2ebc4dea0310"}, "execution_count": 71, "outputs": [{"output_type": "execute_result", "data": {"text/plain": ["                     context_recall  faithfulness  factual_correctness  \\\n", "Base Retriever               0.5417        0.7644                0.340   \n", "Finetuned Retriever          0.8250        0.8179                0.453   \n", "\n", "                     answer_relevancy  context_entity_recall  \\\n", "Base Retriever                 0.4412                 0.6185   \n", "Finetuned Retriever            0.5461                 0.7672   \n", "\n", "                     noise_sensitivity_relevant  \n", "Base Retriever                           0.3479  \n", "Finetuned Retriever                      0.3170  "], "text/html": ["\n", "  <div id=\"df-5af50648-e959-4d7b-a180-01b1e4c2c813\" class=\"colab-df-container\">\n", "    <div>\n", "<style scoped>\n", "    .dataframe tbody tr th:only-of-type {\n", "        vertical-align: middle;\n", "    }\n", "\n", "    .dataframe tbody tr th {\n", "        vertical-align: top;\n", "    }\n", "\n", "    .dataframe thead th {\n", "        text-align: right;\n", "    }\n", "</style>\n", "<table border=\"1\" class=\"dataframe\">\n", "  <thead>\n", "    <tr style=\"text-align: right;\">\n", "      <th></th>\n", "      <th>context_recall</th>\n", "      <th>faithfulness</th>\n", "      <th>factual_correctness</th>\n", "      <th>answer_relevancy</th>\n", "      <th>context_entity_recall</th>\n", "      <th>noise_sensitivity_relevant</th>\n", "    </tr>\n", "  </thead>\n", "  <tbody>\n", "    <tr>\n", "      <th>Base Retriever</th>\n", "      <td>0.5417</td>\n", "      <td>0.7644</td>\n", "      <td>0.340</td>\n", "      <td>0.4412</td>\n", "      <td>0.6185</td>\n", "      <td>0.3479</td>\n", "    </tr>\n", "    <tr>\n", "      <th>Finetuned Retriever</th>\n", "      <td>0.8250</td>\n", "      <td>0.8179</td>\n", "      <td>0.453</td>\n", "      <td>0.5461</td>\n", "      <td>0.7672</td>\n", "      <td>0.3170</td>\n", "    </tr>\n", "  </tbody>\n", "</table>\n", "</div>\n", "    <div class=\"colab-df-buttons\">\n", "\n", "  <div class=\"colab-df-container\">\n", "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5af50648-e959-4d7b-a180-01b1e4c2c813')\"\n", "            title=\"Convert this dataframe to an interactive table.\"\n", "            style=\"display:none;\">\n", "\n", "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n", "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n", "  </svg>\n", "    </button>\n", "\n", "  <style>\n", "    .colab-df-container {\n", "      display:flex;\n", "      gap: 12px;\n", "    }\n", "\n", "    .colab-df-convert {\n", "      background-color: #E8F0FE;\n", "      border: none;\n", "      border-radius: 50%;\n", "      cursor: pointer;\n", "      display: none;\n", "      fill: #1967D2;\n", "      height: 32px;\n", "      padding: 0 0 0 0;\n", "      width: 32px;\n", "    }\n", "\n", "    .colab-df-convert:hover {\n", "      background-color: #E2EBFA;\n", "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n", "      fill: #174EA6;\n", "    }\n", "\n", "    .colab-df-buttons div {\n", "      margin-bottom: 4px;\n", "    }\n", "\n", "    [theme=dark] .colab-df-convert {\n", "      background-color: #3B4455;\n", "      fill: #D2E3FC;\n", "    }\n", "\n", "    [theme=dark] .colab-df-convert:hover {\n", "      background-color: #434B5C;\n", "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n", "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n", "      fill: #FFFFFF;\n", "    }\n", "  </style>\n", "\n", "    <script>\n", "      const buttonEl =\n", "        document.querySelector('#df-5af50648-e959-4d7b-a180-01b1e4c2c813 button.colab-df-convert');\n", "      buttonEl.style.display =\n", "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n", "\n", "      async function convertToInteractive(key) {\n", "        const element = document.querySelector('#df-5af50648-e959-4d7b-a180-01b1e4c2c813');\n", "        const dataTable =\n", "          await google.colab.kernel.invokeFunction('convertToInteractive',\n", "                                                    [key], {});\n", "        if (!dataTable) return;\n", "\n", "        const docLinkHtml = 'Like what you see? Visit the ' +\n", "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n", "          + ' to learn more about interactive tables.';\n", "        element.innerHTML = '';\n", "        dataTable['output_type'] = 'display_data';\n", "        await google.colab.output.renderOutput(dataTable, element);\n", "        const docLink = document.createElement('div');\n", "        docLink.innerHTML = docLinkHtml;\n", "        element.appendChild(docLink);\n", "      }\n", "    </script>\n", "  </div>\n", "\n", "\n", "    <div id=\"df-094e35a2-7753-4ce9-abb1-2030cb00d025\">\n", "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-094e35a2-7753-4ce9-abb1-2030cb00d025')\"\n", "                title=\"Suggest charts\"\n", "                style=\"display:none;\">\n", "\n", "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n", "     width=\"24px\">\n", "    <g>\n", "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n", "    </g>\n", "</svg>\n", "      </button>\n", "\n", "<style>\n", "  .colab-df-quickchart {\n", "      --bg-color: #E8F0FE;\n", "      --fill-color: #1967D2;\n", "      --hover-bg-color: #E2EBFA;\n", "      --hover-fill-color: #174EA6;\n", "      --disabled-fill-color: #AAA;\n", "      --disabled-bg-color: #DDD;\n", "  }\n", "\n", "  [theme=dark] .colab-df-quickchart {\n", "      --bg-color: #3B4455;\n", "      --fill-color: #D2E3FC;\n", "      --hover-bg-color: #434B5C;\n", "      --hover-fill-color: #FFFFFF;\n", "      --disabled-bg-color: #3B4455;\n", "      --disabled-fill-color: #666;\n", "  }\n", "\n", "  .colab-df-quickchart {\n", "    background-color: var(--bg-color);\n", "    border: none;\n", "    border-radius: 50%;\n", "    cursor: pointer;\n", "    display: none;\n", "    fill: var(--fill-color);\n", "    height: 32px;\n", "    padding: 0;\n", "    width: 32px;\n", "  }\n", "\n", "  .colab-df-quickchart:hover {\n", "    background-color: var(--hover-bg-color);\n", "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n", "    fill: var(--button-hover-fill-color);\n", "  }\n", "\n", "  .colab-df-quickchart-complete:disabled,\n", "  .colab-df-quickchart-complete:disabled:hover {\n", "    background-color: var(--disabled-bg-color);\n", "    fill: var(--disabled-fill-color);\n", "    box-shadow: none;\n", "  }\n", "\n", "  .colab-df-spinner {\n", "    border: 2px solid var(--fill-color);\n", "    border-color: transparent;\n", "    border-bottom-color: var(--fill-color);\n", "    animation:\n", "      spin 1s steps(1) infinite;\n", "  }\n", "\n", "  @keyframes spin {\n", "    0% {\n", "      border-color: transparent;\n", "      border-bottom-color: var(--fill-color);\n", "      border-left-color: var(--fill-color);\n", "    }\n", "    20% {\n", "      border-color: transparent;\n", "      border-left-color: var(--fill-color);\n", "      border-top-color: var(--fill-color);\n", "    }\n", "    30% {\n", "      border-color: transparent;\n", "      border-left-color: var(--fill-color);\n", "      border-top-color: var(--fill-color);\n", "      border-right-color: var(--fill-color);\n", "    }\n", "    40% {\n", "      border-color: transparent;\n", "      border-right-color: var(--fill-color);\n", "      border-top-color: var(--fill-color);\n", "    }\n", "    60% {\n", "      border-color: transparent;\n", "      border-right-color: var(--fill-color);\n", "    }\n", "    80% {\n", "      border-color: transparent;\n", "      border-right-color: var(--fill-color);\n", "      border-bottom-color: var(--fill-color);\n", "    }\n", "    90% {\n", "      border-color: transparent;\n", "      border-bottom-color: var(--fill-color);\n", "    }\n", "  }\n", "</style>\n", "\n", "      <script>\n", "        async function quickchart(key) {\n", "          const quickchartButtonEl =\n", "            document.querySelector('#' + key + ' button');\n", "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n", "          quickchartButtonEl.classList.add('colab-df-spinner');\n", "          try {\n", "            const charts = await google.colab.kernel.invokeFunction(\n", "                'suggestCharts', [key], {});\n", "          } catch (error) {\n", "            console.error('Error during call to suggestCharts:', error);\n", "          }\n", "          quickchartButtonEl.classList.remove('colab-df-spinner');\n", "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n", "        }\n", "        (() => {\n", "          let quickchartButtonEl =\n", "            document.querySelector('#df-094e35a2-7753-4ce9-abb1-2030cb00d025 button');\n", "          quickchartButtonEl.style.display =\n", "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n", "        })();\n", "      </script>\n", "    </div>\n", "\n", "  <div id=\"id_718856e3-75bd-4ccf-b623-1e9733baa812\">\n", "    <style>\n", "      .colab-df-generate {\n", "        background-color: #E8F0FE;\n", "        border: none;\n", "        border-radius: 50%;\n", "        cursor: pointer;\n", "        display: none;\n", "        fill: #1967D2;\n", "        height: 32px;\n", "        padding: 0 0 0 0;\n", "        width: 32px;\n", "      }\n", "\n", "      .colab-df-generate:hover {\n", "        background-color: #E2EBFA;\n", "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n", "        fill: #174EA6;\n", "      }\n", "\n", "      [theme=dark] .colab-df-generate {\n", "        background-color: #3B4455;\n", "        fill: #D2E3FC;\n", "      }\n", "\n", "      [theme=dark] .colab-df-generate:hover {\n", "        background-color: #434B5C;\n", "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n", "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n", "        fill: #FFFFFF;\n", "      }\n", "    </style>\n", "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('comparison')\"\n", "            title=\"Generate code using this dataframe.\"\n", "            style=\"display:none;\">\n", "\n", "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n", "       width=\"24px\">\n", "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n", "  </svg>\n", "    </button>\n", "    <script>\n", "      (() => {\n", "      const buttonEl =\n", "        document.querySelector('#id_718856e3-75bd-4ccf-b623-1e9733baa812 button.colab-df-generate');\n", "      buttonEl.style.display =\n", "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n", "\n", "      buttonEl.onclick = () => {\n", "        google.colab.notebook.generateWithVariable('comparison');\n", "      }\n", "      })();\n", "    </script>\n", "  </div>\n", "\n", "    </div>\n", "  </div>\n"], "application/vnd.google.colaboratory.intrinsic+json": {"type": "dataframe", "variable_name": "comparison", "summary": "{\n  \"name\": \"comparison\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"context_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2003233511101489,\n        \"min\": 0.5417,\n        \"max\": 0.825,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.825,\n          0.5417\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"faithfulness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03783021279348029,\n        \"min\": 0.7644,\n        \"max\": 0.8179,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.8179,\n          0.7644\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"factual_correctness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07990306627407986,\n        \"min\": 0.34,\n        \"max\": 0.453,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.453,\n          0.34\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_relevancy\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07417550134646887,\n        \"min\": 0.4412,\n        \"max\": 0.5461,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.5461,\n          0.4412\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context_entity_recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.10514677836243957,\n        \"min\": 0.6185,\n        \"max\": 0.7672,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.7672,\n          0.6185\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"noise_sensitivity_relevant\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.021849599538664305,\n        \"min\": 0.317,\n        \"max\": 0.3479,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.317,\n          0.3479\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}}, "metadata": {}, "execution_count": 71}]}, {"cell_type": "markdown", "source": ["Overall, on every metric, the finetuned embedding model performs better."], "metadata": {"id": "ZG4sIeSsJFSJ"}}], "metadata": {"accelerator": "GPU", "colab": {"gpuType": "L4", "provenance": []}, "kernelspec": {"display_name": "Python 3", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.12.4"}}, "nbformat": 4, "nbformat_minor": 0}